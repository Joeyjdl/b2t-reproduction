{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "33P-pKKroZjV"
   },
   "source": [
    "### Already ran all cells"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vaoCyBv3oZjW"
   },
   "source": [
    "## Setup env and downloads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 33697,
     "status": "ok",
     "timestamp": 1738348359211,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -60
    },
    "id": "v9vdZ1KwoZjX",
    "outputId": "39292bee-3b7f-41fd-f96a-fd6f71d09b88"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading package lists... Done\n",
      "Building dependency tree... Done\n",
      "Reading state information... Done\n",
      "git is already the newest version (1:2.34.1-1ubuntu1.12).\n",
      "0 upgraded, 0 newly installed, 0 to remove and 18 not upgraded.\n",
      "Cloning into 'FACT'...\n",
      "remote: Enumerating objects: 805, done.\u001b[K\n",
      "remote: Counting objects: 100% (146/146), done.\u001b[K\n",
      "remote: Compressing objects: 100% (106/106), done.\u001b[K\n",
      "remote: Total 805 (delta 81), reused 99 (delta 40), pack-reused 659 (from 2)\u001b[K\n",
      "Receiving objects: 100% (805/805), 344.67 MiB | 16.17 MiB/s, done.\n",
      "Resolving deltas: 100% (496/496), done.\n",
      "Updating files: 100% (90/90), done.\n"
     ]
    }
   ],
   "source": [
    "# Install Git if not already installed (only needed in some cases)\n",
    "!apt-get install git\n",
    "\n",
    "# Clone the GitHub repository\n",
    "github_code = \"\"\n",
    "branch_name = \"main\"\n",
    "repo_url = f\"https://{github_code}@github.com/Joeyjdl/FACT.git\"\n",
    "\n",
    "# Clone the repository\n",
    "!git clone -b {branch_name} {repo_url}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 159566,
     "status": "ok",
     "timestamp": 1738348518766,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -60
    },
    "id": "DawZhGlgoZjX",
    "outputId": "32383ce4-1ae7-4f03-88ff-da27a8cfbe62"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/FACT\n",
      "Reading package lists... Done\n",
      "Building dependency tree... Done\n",
      "Reading state information... Done\n",
      "The following additional packages will be installed:\n",
      "  libpython3.8-minimal libpython3.8-stdlib python3.8-minimal\n",
      "Suggested packages:\n",
      "  python3.8-venv binfmt-support\n",
      "The following NEW packages will be installed:\n",
      "  libpython3.8-minimal libpython3.8-stdlib python3.8 python3.8-minimal\n",
      "0 upgraded, 4 newly installed, 0 to remove and 18 not upgraded.\n",
      "Need to get 5,076 kB of archives.\n",
      "After this operation, 18.8 MB of additional disk space will be used.\n",
      "Get:1 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 libpython3.8-minimal amd64 3.8.20-1+jammy1 [796 kB]\n",
      "Get:2 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 python3.8-minimal amd64 3.8.20-1+jammy1 [2,023 kB]\n",
      "Get:3 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 libpython3.8-stdlib amd64 3.8.20-1+jammy1 [1,817 kB]\n",
      "Get:4 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 python3.8 amd64 3.8.20-1+jammy1 [440 kB]\n",
      "Fetched 5,076 kB in 3s (2,010 kB/s)\n",
      "debconf: unable to initialize frontend: Dialog\n",
      "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 4.)\n",
      "debconf: falling back to frontend: Readline\n",
      "debconf: unable to initialize frontend: Readline\n",
      "debconf: (This frontend requires a controlling tty.)\n",
      "debconf: falling back to frontend: Teletype\n",
      "dpkg-preconfigure: unable to re-open stdin: \n",
      "Selecting previously unselected package libpython3.8-minimal:amd64.\n",
      "(Reading database ... 124950 files and directories currently installed.)\n",
      "Preparing to unpack .../libpython3.8-minimal_3.8.20-1+jammy1_amd64.deb ...\n",
      "Unpacking libpython3.8-minimal:amd64 (3.8.20-1+jammy1) ...\n",
      "Selecting previously unselected package python3.8-minimal.\n",
      "Preparing to unpack .../python3.8-minimal_3.8.20-1+jammy1_amd64.deb ...\n",
      "Unpacking python3.8-minimal (3.8.20-1+jammy1) ...\n",
      "Selecting previously unselected package libpython3.8-stdlib:amd64.\n",
      "Preparing to unpack .../libpython3.8-stdlib_3.8.20-1+jammy1_amd64.deb ...\n",
      "Unpacking libpython3.8-stdlib:amd64 (3.8.20-1+jammy1) ...\n",
      "Selecting previously unselected package python3.8.\n",
      "Preparing to unpack .../python3.8_3.8.20-1+jammy1_amd64.deb ...\n",
      "Unpacking python3.8 (3.8.20-1+jammy1) ...\n",
      "Setting up libpython3.8-minimal:amd64 (3.8.20-1+jammy1) ...\n",
      "Setting up python3.8-minimal (3.8.20-1+jammy1) ...\n",
      "Setting up libpython3.8-stdlib:amd64 (3.8.20-1+jammy1) ...\n",
      "Setting up python3.8 (3.8.20-1+jammy1) ...\n",
      "Processing triggers for man-db (2.10.2-1) ...\n",
      "Processing triggers for mailcap (3.70+nmu1ubuntu1) ...\n",
      "Python 3.8.20\n",
      "Reading package lists... Done\n",
      "Building dependency tree... Done\n",
      "Reading state information... Done\n",
      "The following additional packages will be installed:\n",
      "  python3.8-lib2to3\n",
      "The following NEW packages will be installed:\n",
      "  python3.8-distutils python3.8-lib2to3\n",
      "0 upgraded, 2 newly installed, 0 to remove and 18 not upgraded.\n",
      "Need to get 319 kB of archives.\n",
      "After this operation, 1,237 kB of additional disk space will be used.\n",
      "Get:1 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 python3.8-lib2to3 all 3.8.20-1+jammy1 [126 kB]\n",
      "Get:2 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 python3.8-distutils all 3.8.20-1+jammy1 [193 kB]\n",
      "Fetched 319 kB in 2s (193 kB/s)\n",
      "debconf: unable to initialize frontend: Dialog\n",
      "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 2.)\n",
      "debconf: falling back to frontend: Readline\n",
      "debconf: unable to initialize frontend: Readline\n",
      "debconf: (This frontend requires a controlling tty.)\n",
      "debconf: falling back to frontend: Teletype\n",
      "dpkg-preconfigure: unable to re-open stdin: \n",
      "Selecting previously unselected package python3.8-lib2to3.\n",
      "(Reading database ... 125576 files and directories currently installed.)\n",
      "Preparing to unpack .../python3.8-lib2to3_3.8.20-1+jammy1_all.deb ...\n",
      "Unpacking python3.8-lib2to3 (3.8.20-1+jammy1) ...\n",
      "Selecting previously unselected package python3.8-distutils.\n",
      "Preparing to unpack .../python3.8-distutils_3.8.20-1+jammy1_all.deb ...\n",
      "Unpacking python3.8-distutils (3.8.20-1+jammy1) ...\n",
      "Setting up python3.8-lib2to3 (3.8.20-1+jammy1) ...\n",
      "Setting up python3.8-distutils (3.8.20-1+jammy1) ...\n",
      "--2025-01-31 18:32:51--  https://bootstrap.pypa.io/get-pip.py\n",
      "Resolving bootstrap.pypa.io (bootstrap.pypa.io)... 151.101.0.175, 151.101.64.175, 151.101.128.175, ...\n",
      "Connecting to bootstrap.pypa.io (bootstrap.pypa.io)|151.101.0.175|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 2275758 (2.2M) [text/x-python]\n",
      "Saving to: ‘get-pip.py’\n",
      "\n",
      "get-pip.py          100%[===================>]   2.17M  --.-KB/s    in 0.01s   \n",
      "\n",
      "2025-01-31 18:32:51 (204 MB/s) - ‘get-pip.py’ saved [2275758/2275758]\n",
      "\n",
      "Collecting pip\n",
      "  Downloading pip-25.0-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting setuptools\n",
      "  Downloading setuptools-75.3.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Collecting wheel\n",
      "  Downloading wheel-0.45.1-py3-none-any.whl.metadata (2.3 kB)\n",
      "Downloading pip-25.0-py3-none-any.whl (1.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m52.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading setuptools-75.3.0-py3-none-any.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m57.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading wheel-0.45.1-py3-none-any.whl (72 kB)\n",
      "Installing collected packages: wheel, setuptools, pip\n",
      "Successfully installed pip-25.0 setuptools-75.3.0 wheel-0.45.1\n",
      "Collecting pipenv\n",
      "  Downloading pipenv-2024.4.1-py3-none-any.whl.metadata (17 kB)\n",
      "Collecting certifi (from pipenv)\n",
      "  Downloading certifi-2025.1.31-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting packaging>=22 (from pipenv)\n",
      "  Downloading packaging-24.2-py3-none-any.whl.metadata (3.2 kB)\n",
      "Requirement already satisfied: setuptools>=67 in /usr/local/lib/python3.8/dist-packages (from pipenv) (75.3.0)\n",
      "Collecting virtualenv>=20.24.2 (from pipenv)\n",
      "  Downloading virtualenv-20.29.1-py3-none-any.whl.metadata (4.5 kB)\n",
      "Collecting distlib<1,>=0.3.7 (from virtualenv>=20.24.2->pipenv)\n",
      "  Downloading distlib-0.3.9-py2.py3-none-any.whl.metadata (5.2 kB)\n",
      "Collecting filelock<4,>=3.12.2 (from virtualenv>=20.24.2->pipenv)\n",
      "  Downloading filelock-3.16.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting platformdirs<5,>=3.9.1 (from virtualenv>=20.24.2->pipenv)\n",
      "  Downloading platformdirs-4.3.6-py3-none-any.whl.metadata (11 kB)\n",
      "Downloading pipenv-2024.4.1-py3-none-any.whl (3.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m105.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading packaging-24.2-py3-none-any.whl (65 kB)\n",
      "Downloading virtualenv-20.29.1-py3-none-any.whl (4.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.3/4.3 MB\u001b[0m \u001b[31m119.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading certifi-2025.1.31-py3-none-any.whl (166 kB)\n",
      "Downloading distlib-0.3.9-py2.py3-none-any.whl (468 kB)\n",
      "Downloading filelock-3.16.1-py3-none-any.whl (16 kB)\n",
      "Downloading platformdirs-4.3.6-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: distlib, platformdirs, packaging, filelock, certifi, virtualenv, pipenv\n",
      "Successfully installed certifi-2025.1.31 distlib-0.3.9 filelock-3.16.1 packaging-24.2 pipenv-2024.4.1 platformdirs-4.3.6 virtualenv-20.29.1\n",
      "\u001b[1mCreating a virtualenv for this project\u001b[0m\n",
      "Pipfile: \u001b[1;33m/content/FACT/\u001b[0m\u001b[1;33mPipfile\u001b[0m\n",
      "\u001b[1mUsing\u001b[0m \u001b[1;33m/usr/bin/\u001b[0m\u001b[1;33mpython3.8\u001b[0m\u001b[32m3.8.20\u001b[0m\u001b[32m \u001b[0m\u001b[1;32mto create virtualenv\u001b[0m\u001b[1;32m...\u001b[0m\n",
      "\u001b[2K\u001b[32m⠸\u001b[0m Creating virtual environment...\u001b[36mcreated virtual environment CPython3.\u001b[0m\u001b[1;36m8.20\u001b[0m\u001b[36m.final.\u001b[0m\u001b[1;36m0\u001b[0m\u001b[36m-\u001b[0m\u001b[1;36m64\u001b[0m\u001b[36m in 908ms\u001b[0m\n",
      "\u001b[36m  creator \u001b[0m\u001b[1;36mCPython3Posix\u001b[0m\u001b[1;36m(\u001b[0m\u001b[36mdest\u001b[0m\u001b[36m=\u001b[0m\u001b[36m/root/.local/share/virtualenvs/\u001b[0m\u001b[36mFACT-Z5lTYn3d\u001b[0m\u001b[36m, \u001b[0m\u001b[36mclear\u001b[0m\u001b[36m=\u001b[0m\u001b[3;36mFalse\u001b[0m\u001b[36m, \u001b[0m\n",
      "\u001b[36mno_vcs_ignore\u001b[0m\u001b[36m=\u001b[0m\u001b[3;36mFalse\u001b[0m\u001b[36m, \u001b[0m\u001b[36mglobal\u001b[0m\u001b[36m=\u001b[0m\u001b[3;36mFalse\u001b[0m\u001b[1;36m)\u001b[0m\n",
      "\u001b[36m  seeder \u001b[0m\u001b[1;36mFromAppData\u001b[0m\u001b[1;36m(\u001b[0m\u001b[36mdownload\u001b[0m\u001b[36m=\u001b[0m\u001b[3;36mFalse\u001b[0m\u001b[36m, \u001b[0m\u001b[36mpip\u001b[0m\u001b[36m=\u001b[0m\u001b[36mbundle\u001b[0m\u001b[36m, \u001b[0m\u001b[36msetuptools\u001b[0m\u001b[36m=\u001b[0m\u001b[36mbundle\u001b[0m\u001b[36m, \u001b[0m\u001b[36mwheel\u001b[0m\u001b[36m=\u001b[0m\u001b[36mbundle\u001b[0m\u001b[36m, \u001b[0m\u001b[36mvia\u001b[0m\u001b[36m=\u001b[0m\u001b[36mcopy\u001b[0m\u001b[36m, \u001b[0m\n",
      "\u001b[36mapp_data_dir\u001b[0m\u001b[36m=\u001b[0m\u001b[36m/root/.local/share/\u001b[0m\u001b[36mvirtualenv\u001b[0m\u001b[1;36m)\u001b[0m\n",
      "\u001b[36m    added seed packages: \u001b[0m\u001b[36mpip\u001b[0m\u001b[36m==\u001b[0m\u001b[1;36m24.3\u001b[0m\u001b[36m.\u001b[0m\u001b[1;36m1\u001b[0m\u001b[36m, \u001b[0m\u001b[36msetuptools\u001b[0m\u001b[36m==\u001b[0m\u001b[1;36m75.3\u001b[0m\u001b[36m.\u001b[0m\u001b[1;36m0\u001b[0m\u001b[36m, \u001b[0m\u001b[36mwheel\u001b[0m\u001b[36m==\u001b[0m\u001b[1;36m0.45\u001b[0m\u001b[36m.\u001b[0m\u001b[1;36m1\u001b[0m\n",
      "\u001b[36m  activators \u001b[0m\n",
      "\u001b[36mBashActivator,CShellActivator,FishActivator,NushellActivator,PowerShellActivator,PythonActivator\u001b[0m\n",
      "\n",
      "✔ Successfully created virtual environment!\n",
      "\u001b[2K\u001b[32m⠼\u001b[0m Creating virtual environment...\n",
      "\u001b[1A\u001b[2K\u001b[32mVirtualenv location: \u001b[0m\u001b[32m/root/.local/share/virtualenvs/\u001b[0m\u001b[32mFACT-Z5lTYn3d\u001b[0m\n",
      "\u001b[1;33mPipfile.lock \u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m5546551674a7d08c2eaa76bb0dbafa467082548285cf37934da2656706ec2fee\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m out of date: run \u001b[0m\n",
      "\u001b[1;33m`pipenv lock` to update to \u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m6d0072f85315a41cc2e20ba8cbe06b6836f1756003c8dcf26d394ddaac16ddc6\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m...\u001b[0m\n",
      "Locking\u001b[0m \u001b[33m[packages]\u001b[0m dependencies...\u001b[0m\n",
      "\u001b[?25lBuilding requirements...\n",
      "\u001b[2KResolving dependencies...\n",
      "\u001b[2K✔ Success!\n",
      "\u001b[32m⠦\u001b[0m Locking packages...Warning: INFO:pip.subprocessor:Running command git clone --\u001b[33mfilter\u001b[0m=\u001b[35mblob\u001b[0m:none --quiet \u001b[4;94mhttps://github.com/openai/CLIP.git\u001b[0m \u001b[35m/tmp/pip-temp-g6txtrpp/\u001b[0m\u001b[95mclip_e09bc840e55b4045ad631e9556f194ec\u001b[0m\n",
      "INFO:pip.subprocessor:Running command git clone --\u001b[33mfilter\u001b[0m=\u001b[35mblob\u001b[0m:none --quiet \u001b[4;94mhttps://github.com/ml-research/semantic-image-editing.git\u001b[0m \u001b[35m/tmp/pip-temp-g6txtrpp/\u001b[0m\u001b[95msemdiffusers_d08166f8c04443968d022661fe2cb8d4\u001b[0m\n",
      "INFO:pip.subprocessor:Running command git clone --\u001b[33mfilter\u001b[0m=\u001b[35mblob\u001b[0m:none \u001b[4;94mhttps://github.com/ml-research/semantic-image-editing.git\u001b[0m \u001b[35m/tmp/\u001b[0m\u001b[95mtmpjk5251i7\u001b[0m\n",
      "INFO:pip.subprocessor:Cloning into \u001b[32m'/tmp/tmpjk5251i7'\u001b[0m\u001b[33m...\u001b[0m\n",
      "INFO:pip.subprocessor:Updating files:  \u001b[1;36m20\u001b[0m% \u001b[1m(\u001b[0m\u001b[1;36m2\u001b[0m/\u001b[1;36m10\u001b[0m\u001b[1m)\u001b[0m\n",
      "INFO:pip.subprocessor:Updating files:  \u001b[1;36m30\u001b[0m% \u001b[1m(\u001b[0m\u001b[1;36m3\u001b[0m/\u001b[1;36m10\u001b[0m\u001b[1m)\u001b[0m\n",
      "INFO:pip.subprocessor:Updating files:  \u001b[1;36m40\u001b[0m% \u001b[1m(\u001b[0m\u001b[1;36m4\u001b[0m/\u001b[1;36m10\u001b[0m\u001b[1m)\u001b[0m\n",
      "INFO:pip.subprocessor:Updating files:  \u001b[1;36m50\u001b[0m% \u001b[1m(\u001b[0m\u001b[1;36m5\u001b[0m/\u001b[1;36m10\u001b[0m\u001b[1m)\u001b[0m\n",
      "INFO:pip.subprocessor:Updating files:  \u001b[1;36m60\u001b[0m% \u001b[1m(\u001b[0m\u001b[1;36m6\u001b[0m/\u001b[1;36m10\u001b[0m\u001b[1m)\u001b[0m\n",
      "INFO:pip.subprocessor:Updating files:  \u001b[1;36m70\u001b[0m% \u001b[1m(\u001b[0m\u001b[1;36m7\u001b[0m/\u001b[1;36m10\u001b[0m\u001b[1m)\u001b[0m\n",
      "INFO:pip.subprocessor:Updating files:  \u001b[1;36m80\u001b[0m% \u001b[1m(\u001b[0m\u001b[1;36m8\u001b[0m/\u001b[1;36m10\u001b[0m\u001b[1m)\u001b[0m\n",
      "INFO:pip.subprocessor:Updating files:  \u001b[1;36m90\u001b[0m% \u001b[1m(\u001b[0m\u001b[1;36m9\u001b[0m/\u001b[1;36m10\u001b[0m\u001b[1m)\u001b[0m\n",
      "INFO:pip.subprocessor:Updating files: \u001b[1;36m100\u001b[0m% \u001b[1m(\u001b[0m\u001b[1;36m10\u001b[0m/\u001b[1;36m10\u001b[0m\u001b[1m)\u001b[0m\n",
      "INFO:pip.subprocessor:Updating files: \u001b[1;36m100\u001b[0m% \u001b[1m(\u001b[0m\u001b[1;36m10\u001b[0m/\u001b[1;36m10\u001b[0m\u001b[1m)\u001b[0m, done.\n",
      "INFO:pip.subprocessor:Running command git clone --\u001b[33mfilter\u001b[0m=\u001b[35mblob\u001b[0m:none \u001b[4;94mhttps://github.com/openai/CLIP.git\u001b[0m \u001b[35m/tmp/\u001b[0m\u001b[95mtmpfof_u_gl\u001b[0m\n",
      "INFO:pip.subprocessor:Cloning into \u001b[32m'/tmp/tmpfof_u_gl'\u001b[0m\u001b[33m...\u001b[0m\n",
      "\u001b[2K\u001b[32m⠦\u001b[0m Locking packages...\n",
      "\u001b[1A\u001b[2KLocking\u001b[0m \u001b[33m[dev-packages]\u001b[0m dependencies...\u001b[0m\n",
      "\u001b[1mUpdated Pipfile.lock (6d0072f85315a41cc2e20ba8cbe06b6836f1756003c8dcf26d394ddaac16ddc6)!\u001b[0m\n",
      "To activate this project's virtualenv, run \u001b[33mpipenv shell\u001b[0m.\n",
      "Alternatively, run a command inside the virtualenv with \u001b[33mpipenv run\u001b[0m.\n",
      "\u001b[1mInstalling dependencies from Pipfile.lock \u001b[0m\u001b[1;39m(16ddc6)...\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%cd /content/FACT\n",
    "!sudo apt-get install python3.8\n",
    "!sudo update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.8 1\n",
    "!python3.8 --version\n",
    "\n",
    "!sudo apt-get install python3.8-distutils\n",
    "!wget https://bootstrap.pypa.io/get-pip.py\n",
    "!python3.8 get-pip.py\n",
    "!python3.8 -m pip install pipenv\n",
    "!pipenv --python 3.8\n",
    "!pipenv install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1474,
     "status": "ok",
     "timestamp": 1738349632951,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -60
    },
    "id": "r3_6lROpoZjX",
    "outputId": "41c8ec11-53c9-4061-8eb8-e5a0b312872e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/FACT\n",
      "File 'clipcap.pt' already exists.\n",
      "File 'best_model_Waterbirds_erm.pth' already exists.\n",
      "File 'best_model_Waterbirds_dro.pth' already exists.\n",
      "File 'best_model_CelebA_erm.pth' already exists.\n",
      "File 'best_model_CelebA_dro.pth' already exists.\n",
      "File 'waterbirds.tar.gz' or Folder 'waterbird_complete95_forest2water2' already exists.\n",
      "Folder 'waterbird_complete95_forest2water2' already exists.\n",
      "File 'celebA.tar.gz' or Folder 'img_align_celeba' already exists.\n",
      "Folder 'img_align_celeba' already exists.\n",
      "Fairfaces dataset already downloaded and organized.\n"
     ]
    }
   ],
   "source": [
    "%cd /content/FACT\n",
    "\n",
    "# Options are 'celeba', 'waterbirds', 'fairfaces', 'imagenet'\n",
    "dataset = \"imagenet\" # Space-separated list of datasets\n",
    "\n",
    "# Only caption model is clipcap\n",
    "caption_model = \"clipcap\"  # Space-separated list of caption models\n",
    "\n",
    "# Options are 'best_model_CelebA_erm.pth', 'best_model_CelebA_dro.pth', 'best_model_Waterbirds_erm.pth', 'best_model_Waterbirds_dro.pth'\n",
    "classification_model = \"\"  # Space-separated list of classification models\n",
    "\n",
    "# When true downloads everything regardless of other settings\n",
    "download_all = \"True\"  # Set to \"True\" or \"False\"\n",
    "\n",
    "# Build the command with variables\n",
    "command = \"python Downloader.py\"\n",
    "\n",
    "if dataset:\n",
    "    command += f\" --dataset {dataset}\"\n",
    "if caption_model:\n",
    "    command += f\" --caption_model {caption_model}\"\n",
    "if classification_model:\n",
    "    command += f\" --classification_model {classification_model}\"\n",
    "if download_all and download_all.lower() == \"true\":\n",
    "    command += \" --download_all\"\n",
    "\n",
    "!pipenv run bash -c \"{command}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aHYeqRBYoZjY"
   },
   "source": [
    "## With variables for personal running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_pqNgrRzoZjY"
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "# Change working directory\n",
    "%cd /content/FACT/b2t\n",
    "\n",
    "# Define dataset and corresponding models correctly\n",
    "datasets = {\n",
    "    \"celeba\": {\n",
    "        \"dro\": \"best_model_CelebA_dro.pth\",\n",
    "        \"erm\": \"best_model_CelebA_erm.pth\",\n",
    "    },\n",
    "    \"waterbird\": {\n",
    "        \"dro\": \"best_model_Waterbirds_dro.pth\",\n",
    "        \"erm\": \"best_model_Waterbirds_erm.pth\",\n",
    "    },\n",
    "    \"imagenet\": {\n",
    "        \"resnet50\": \"Resnet50\",\n",
    "    },\n",
    "}\n",
    "\n",
    "# Set to true to use no classifier\n",
    "no_model = False\n",
    "\n",
    "# Define variables that get used in function call\n",
    "dataset = \"celeba\"  # Change as needed\n",
    "\n",
    "# Model type for celeb and waterbirds\n",
    "model_type = \"dro\"   # Choose between \"dro\" and \"erm\"\n",
    "\n",
    "# List of names for imagenet\n",
    "class_names = [\"bee\", \"ant\"]  # Modify as needed, or set to None\n",
    "\n",
    "# Set to true to extract the captions\n",
    "extract_caption = True\n",
    "\n",
    "# Set to true to save the results after running\n",
    "save_result = True\n",
    "\n",
    "# Set to true to override the saved results if needed\n",
    "rerun_result = False\n",
    "\n",
    "# Update the results without needed to use the captions again\n",
    "override_result = False\n",
    "\n",
    "# Set the minority ratio for fair faces\n",
    "minority_ratio = 0.05\n",
    "\n",
    "# Set the minority class 0: white male, 1: black male, 2: white female and 3: black female\n",
    "minority_group = 3\n",
    "\n",
    "\n",
    "# Get the model name\n",
    "model = datasets[dataset][model_type]\n",
    "\n",
    "# Construct the command dynamically\n",
    "command = f\"python b2t.py --dataset {dataset} --model {model}\"\n",
    "\n",
    "if extract_caption:\n",
    "    command += \" --extract_caption\"\n",
    "\n",
    "if class_names:\n",
    "    command += \" --class_names \" + \" \".join(class_names)\n",
    "\n",
    "if save_result:\n",
    "    command += \" --save_result\"\n",
    "\n",
    "if rerun_result:\n",
    "    command += \" --rerun_result\"\n",
    "\n",
    "if override_result:\n",
    "    command += \" --override_result\"\n",
    "\n",
    "if no_model:\n",
    "    command += \" --no_model\"\n",
    "\n",
    "# Add optional numerical parameters\n",
    "command += f\" --minority_ratio {minority_ratio}\"\n",
    "command += f\" --minority_group {minority_group}\"\n",
    "\n",
    "# Run the command\n",
    "!pipenv run bash -c \"{command}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "do-9i8MOoZjY"
   },
   "source": [
    "## No class keyword method ran on fairfaces, celeba and waterbird datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 166087,
     "status": "ok",
     "timestamp": 1738351644187,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -60
    },
    "id": "VFov5sOiPj8w",
    "outputId": "e059cdfd-efe4-4b5f-f870-5d423ff75af1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/FACT/b2t\n",
      "Recommend running fairfaces initially with -1 as minority_ratio and extract_caption , allowing you to only have to extract_caption once! (all data will be captioned). Runs after this may use --override_result\n",
      "Start extracting captions...\n",
      "Processing Images: 100% 3641/3641 [01:42<00:00, 35.57image/s]\n",
      "Captions for 3641 images have been extracted.\n",
      "100% 15/15 [00:12<00:00,  1.16it/s]\n",
      "# of correct examples :  3641\n",
      "# of wrong examples :  0\n",
      "# of all examples :  3641\n",
      "Accuracy : 100.00 %\n",
      "Classified result stored\n",
      "Start calculating scores..\n",
      "df_wrong_class_0[image] count: 1122\n",
      "df_correct_class_0[image] count: 799\n",
      "keywords_class_0 count: 40\n",
      "df_wrong_class_1[image] count: 1\n",
      "df_correct_class_1[image] count: 1719\n",
      "keywords_class_1 count: 40\n",
      "Processing images: 100% 18/18 [00:05<00:00,  3.51it/s]\n",
      "Processing images: 100% 13/13 [00:03<00:00,  4.16it/s]\n",
      "Processing images: 100% 1/1 [00:00<00:00, 15.49it/s]\n",
      "Processing images: 100% 27/27 [00:06<00:00,  4.09it/s]\n",
      "similarity_wrong_class_0: tensor([24.1875, 23.8281, 21.8125, 23.0938, 22.8906, 23.1406, 20.6406, 22.7969,\n",
      "        23.8594, 20.5000, 21.3750, 22.2500, 23.5312, 23.2031, 20.4219, 17.6406,\n",
      "        19.0000, 22.1875, 22.5000, 22.8125, 22.5781, 22.2812, 20.2188, 24.0469,\n",
      "        21.6719, 19.8125, 23.7812, 22.3594, 20.0469, 22.6250, 22.5938, 22.0469,\n",
      "        21.8906, 22.7969, 22.5625, 22.0156, 22.5312, 20.5312, 23.6250, 23.3125],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "similarity_wrong_class_0: tensor([24.0312, 23.3438, 21.0625, 23.5000, 22.5781, 22.7344, 20.0000, 23.1562,\n",
      "        23.5000, 19.8906, 20.1875, 21.8125, 22.8281, 23.4531, 21.0000, 16.8438,\n",
      "        18.5312, 21.7344, 23.1875, 25.5938, 22.6562, 21.7500, 20.2969, 23.5156,\n",
      "        20.5469, 19.8594, 23.9531, 22.8594, 19.4219, 22.7969, 21.9531, 23.3906,\n",
      "        21.5000, 22.8125, 22.2188, 21.5000, 22.0625, 19.7812, 22.9688, 22.6250],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "similarity_wrong_class_0: tensor([23.7969, 20.7969, 21.9688, 23.8281, 24.4375, 21.1250, 22.8125, 22.1719,\n",
      "        20.4219, 24.2031, 21.2031, 20.0625, 17.8281, 22.3281, 21.7031, 21.7812,\n",
      "        21.0781, 24.8125, 23.0469, 22.2969, 19.0469, 16.5625, 20.4844, 25.2656,\n",
      "        19.9688, 25.4219, 20.6250, 22.4844, 21.8906, 16.8750, 20.4219, 21.8906,\n",
      "        21.7344, 25.2188, 19.0625, 19.8281, 21.1719, 23.0781, 21.1719, 18.8594],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "similarity_wrong_class_0: tensor([24.0156, 21.2969, 23.4844, 23.5938, 23.8906, 21.7812, 23.6719, 23.0312,\n",
      "        22.3594, 23.7031, 22.2031, 21.1719, 18.2188, 22.9688, 22.3281, 22.1875,\n",
      "        20.8906, 22.5156, 22.5625, 23.2344, 19.6094, 16.9844, 23.2656, 23.6562,\n",
      "        20.1406, 24.2812, 19.8906, 23.4062, 22.5312, 18.2344, 20.9688, 22.0156,\n",
      "        22.2188, 23.4844, 21.5000, 21.4375, 20.1875, 23.0625, 23.6562, 21.4375],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "Result for class : male\n",
      "Keyword                  Score    Acc.  Bias\n",
      "-------------------  ---------  ------  ------\n",
      "beard                 1.1875         1  S\n",
      "punched               1.125          1  S\n",
      "bedroom               0.796875       1  S\n",
      "photo                 0.75           1  S\n",
      "found                 0.75           1  S\n",
      "person was found      0.703125       1  S\n",
      "genetic               0.6875         1  S\n",
      "allegedly punched     0.65625        1  S\n",
      "shot                  0.640625       1  S\n",
      "missing               0.640625       1  S\n",
      "fight                 0.625          1  S\n",
      "camera                0.609375       1  S\n",
      "allegedly             0.53125        1  S\n",
      "diagnosed             0.53125        1  S\n",
      "punched a woman       0.515625       1  S\n",
      "man                   0.484375       1  S\n",
      "car                   0.46875        1  S\n",
      "shown                 0.46875        1  S\n",
      "condition             0.453125       1  S\n",
      "smiles                0.4375         1  M\n",
      "head                  0.40625        1  S\n",
      "rare                  0.390625       1  S\n",
      "face                  0.359375       1  M\n",
      "smile                 0.34375        1  S\n",
      "arrested              0.3125         1  S\n",
      "person                0.15625        1  S\n",
      "young                -0.015625       1\n",
      "tooth                -0.046875       1\n",
      "undated              -0.078125       1\n",
      "person was arrested  -0.078125       1\n",
      "person smiles        -0.171875       1\n",
      "native               -0.171875       1\n",
      "boy smiles           -0.25           1\n",
      "student              -0.359375       1\n",
      "boy                  -0.40625        1\n",
      "born                 -0.5            1\n",
      "woman                -0.578125       1\n",
      "child                -0.6875         1\n",
      "black                -1.34375        1\n",
      "black person         -2.78125        1\n",
      "************************************************************\n"
     ]
    }
   ],
   "source": [
    "# first run  this to get all captions for fairfaces, ignore output\n",
    "\n",
    "%cd /content/FACT/b2t\n",
    "# Define the Pipenv command and script\n",
    "command = \"python b2t.py --dataset fairfaces  --model best_model_Waterbirds_erm.pth --minority_group 0 --minority_ratio -1 --extract_caption --no_model\"\n",
    "# Run the command using the Pipenv environment\n",
    "!pipenv run bash -c \"{command}\"\n",
    "\n",
    "#, then run following cells with subset minorities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 54152,
     "status": "ok",
     "timestamp": 1738351698329,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -60
    },
    "id": "8Bs8nmvW5TQ8",
    "outputId": "b61f0e25-d9e6-4a94-953d-46299aed4e9a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/FACT/b2t\n",
      "fairfaces group 0 as minority, 3% like waterbirds. OUR METHOD\n",
      "Recommend running fairfaces initially with -1 as minority_ratio and extract_caption , allowing you to only have to extract_caption once! (all data will be captioned). Runs after this may use --override_result\n",
      "Classified result \"result/fairfaces_no_model.csv\" loaded\n",
      "100% 11/11 [00:07<00:00,  1.38it/s]\n",
      "Session will use filtered indices based on input arguments\n",
      "Start calculating scores..\n",
      "df_wrong_class_0[image] count: 789\n",
      "df_correct_class_0[image] count: 583\n",
      "keywords_class_0 count: 40\n",
      "df_wrong_class_1[image] count: 1\n",
      "df_correct_class_1[image] count: 1223\n",
      "keywords_class_1 count: 40\n",
      "Processing images: 100% 13/13 [00:03<00:00,  3.93it/s]\n",
      "Processing images: 100% 10/10 [00:02<00:00,  4.15it/s]\n",
      "Processing images: 100% 1/1 [00:00<00:00, 17.70it/s]\n",
      "Processing images: 100% 20/20 [00:04<00:00,  4.01it/s]\n",
      "similarity_wrong_class_0: tensor([24.2031, 23.8750, 21.8438, 23.1094, 22.9219, 23.1562, 20.6562, 23.8750,\n",
      "        22.8281, 20.5156, 21.4688, 22.2969, 20.3906, 23.5781, 19.0000, 23.2500,\n",
      "        22.5000, 22.1875, 17.6562, 22.6094, 22.2969, 19.8281, 24.0781, 20.2031,\n",
      "        22.8594, 21.6875, 20.0781, 22.6406, 23.7031, 22.6250, 22.6250, 22.3750,\n",
      "        23.6250, 22.6094, 23.8438, 22.0156, 21.6875, 22.0781, 21.9219, 23.3438],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "similarity_wrong_class_0: tensor([24.0000, 23.3281, 21.0469, 23.4688, 22.5625, 22.7031, 19.9844, 23.4688,\n",
      "        23.1406, 19.8750, 20.1562, 21.7656, 20.9531, 22.7969, 18.5000, 23.4062,\n",
      "        23.1562, 21.7188, 16.8594, 22.6562, 21.7500, 19.7969, 23.5000, 20.2812,\n",
      "        25.5625, 20.5312, 19.4219, 22.7812, 23.4219, 21.9531, 21.6719, 22.8125,\n",
      "        22.9688, 22.1875, 23.9062, 21.5000, 20.4844, 23.3438, 21.4688, 22.5938],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "similarity_wrong_class_0: tensor([23.7969, 20.7969, 21.9688, 23.8281, 24.4375, 21.1250, 22.8125, 20.4219,\n",
      "        22.1719, 21.2031, 24.2031, 20.0625, 22.3281, 21.7812, 17.8281, 21.7031,\n",
      "        21.0781, 23.0469, 16.5625, 24.8125, 22.2969, 19.0469, 20.4844, 20.6250,\n",
      "        16.8750, 19.9688, 21.8906, 20.4219, 22.4844, 25.2656, 21.8906, 25.2188,\n",
      "        21.7344, 19.0625, 18.8594, 23.0781, 25.4219, 19.8281, 21.1719, 18.4531],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "similarity_wrong_class_0: tensor([24.0156, 21.2969, 23.4844, 23.5938, 23.8750, 21.7656, 23.6719, 22.3438,\n",
      "        23.0156, 22.2031, 23.6875, 21.1562, 22.9531, 22.1875, 18.2188, 22.3281,\n",
      "        20.8906, 22.5469, 17.0000, 22.5000, 23.2500, 19.6094, 23.2812, 19.8594,\n",
      "        18.2344, 20.1250, 22.5156, 20.9688, 23.4062, 23.6562, 22.0000, 23.4844,\n",
      "        22.2031, 21.5156, 21.4375, 23.0469, 24.2812, 21.4219, 20.1719, 20.5625],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "Result for class : male\n",
      "Keyword                  Score    Acc.  Bias\n",
      "-------------------  ---------  ------  ------\n",
      "beard                 1.3125         1  S\n",
      "broken                1.20312        1  M\n",
      "punched               1.15625        1  S\n",
      "died                  0.953125       1  S\n",
      "bedroom               0.796875       1  S\n",
      "found                 0.796875       1  S\n",
      "person was found      0.78125        1  S\n",
      "genetic               0.75           1  S\n",
      "shot                  0.671875       1  S\n",
      "missing               0.671875       1  S\n",
      "allegedly punched     0.65625        1  S\n",
      "fight                 0.65625        1  S\n",
      "camera                0.640625       1  S\n",
      "allegedly             0.578125       1  S\n",
      "man                   0.546875       1  S\n",
      "diagnosed             0.546875       1  S\n",
      "smiles                0.53125        1  M\n",
      "punched a woman       0.515625       1  S\n",
      "car                   0.5            1  S\n",
      "condition             0.46875        1  S\n",
      "rare                  0.453125       1  S\n",
      "head                  0.453125       1  S\n",
      "smile                 0.421875       1  S\n",
      "face                  0.40625        1  M\n",
      "arrested              0.359375       1  S\n",
      "player                0.28125        1  S\n",
      "person                0.203125       1  S\n",
      "tooth                 0.03125        1  S\n",
      "person was arrested  -0.046875       1\n",
      "person smiles        -0.0625         1\n",
      "undated              -0.078125       1\n",
      "native               -0.140625       1\n",
      "boy smiles           -0.15625        1\n",
      "student              -0.3125         1\n",
      "boy                  -0.359375       1\n",
      "born                 -0.4375         1\n",
      "woman                -0.5625         1\n",
      "child                -0.65625        1\n",
      "black                -1.26562        1\n",
      "black person         -2.70312        1\n",
      "************************************************************\n"
     ]
    }
   ],
   "source": [
    "%cd /content/FACT/b2t\n",
    "print(\"fairfaces group 0 as minority, 3% like waterbirds. OUR METHOD\")\n",
    "\n",
    "\n",
    "# Define the Pipenv command and script\n",
    "command = \"python b2t.py --dataset fairfaces  --model best_model_Waterbirds_erm.pth --minority_group 0 --minority_ratio 0.03 --override_result --no_model\"\n",
    "# Run the command using the Pipenv environment\n",
    "!pipenv run bash -c \"{command}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 56106,
     "status": "ok",
     "timestamp": 1738351754425,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -60
    },
    "id": "x3fl1tbt5oHb",
    "outputId": "6a5f0c0a-24ba-4d61-f708-dede4b439a25"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/FACT/b2t\n",
      "fairfaces group 1 as minority, 3% like waterbirds. OUR METHOD\n",
      "Recommend running fairfaces initially with -1 as minority_ratio and extract_caption , allowing you to only have to extract_caption once! (all data will be captioned). Runs after this may use --override_result\n",
      "Classified result \"result/fairfaces_no_model.csv\" loaded\n",
      "100% 12/12 [00:08<00:00,  1.37it/s]\n",
      "Session will use filtered indices based on input arguments\n",
      "Start calculating scores..\n",
      "df_wrong_class_0[image] count: 650\n",
      "df_correct_class_0[image] count: 902\n",
      "keywords_class_0 count: 40\n",
      "df_wrong_class_1[image] count: 1\n",
      "df_correct_class_1[image] count: 1376\n",
      "keywords_class_1 count: 40\n",
      "Processing images: 100% 11/11 [00:02<00:00,  3.70it/s]\n",
      "Processing images: 100% 15/15 [00:04<00:00,  3.55it/s]\n",
      "Processing images: 100% 1/1 [00:00<00:00, 17.66it/s]\n",
      "Processing images: 100% 22/22 [00:06<00:00,  3.59it/s]\n",
      "similarity_wrong_class_0: tensor([24.0156, 21.0625, 23.3438, 22.7188, 18.5312, 21.7500, 20.1562, 22.5781,\n",
      "        23.5000, 21.7656, 23.1562, 23.2812, 21.6875, 23.4844, 22.9531, 21.4844,\n",
      "        23.1250, 23.2500, 22.8438, 21.4062, 22.6094, 22.0781, 19.9375, 20.0156,\n",
      "        22.0469, 22.7969, 19.4219, 21.0000, 18.9375, 16.8438, 23.3438, 21.1250,\n",
      "        22.0781, 23.3438, 20.2969, 21.2344, 21.3750, 21.8594, 17.3125, 21.9219],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "similarity_wrong_class_0: tensor([24.1875, 21.8281, 23.8438, 23.1406, 18.9844, 22.1719, 21.4375, 22.8906,\n",
      "        23.8594, 22.2812, 22.8125, 23.6719, 22.6094, 23.1094, 23.7031, 21.9062,\n",
      "        23.6875, 23.5938, 22.3594, 22.1875, 23.3281, 22.8906, 19.6719, 20.6562,\n",
      "        22.4219, 23.5469, 20.0469, 20.3906, 20.0781, 17.6406, 23.6406, 22.0469,\n",
      "        22.5312, 22.9375, 20.2188, 21.8906, 21.6562, 22.2656, 17.6719, 22.0469],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "similarity_wrong_class_0: tensor([23.7969, 20.7969, 21.9688, 23.8281, 24.4375, 21.1250, 22.8125, 22.1719,\n",
      "        20.4219, 24.2031, 21.2031, 20.0625, 17.8281, 22.3281, 21.7031, 21.7812,\n",
      "        21.0781, 23.0469, 24.8125, 16.5625, 22.2969, 19.0469, 20.4844, 19.9688,\n",
      "        20.6250, 22.4844, 21.8906, 16.8750, 25.2656, 20.4219, 25.2188, 19.0625,\n",
      "        21.8906, 21.7344, 23.0781, 18.8594, 19.8281, 21.1719, 21.1719, 24.9844],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "similarity_wrong_class_0: tensor([24.0156, 21.3125, 23.4688, 23.5938, 23.9062, 21.7812, 23.6719, 23.0312,\n",
      "        22.3438, 23.7031, 22.2188, 21.1719, 18.2344, 22.9531, 22.3438, 22.1875,\n",
      "        20.9062, 22.5469, 22.5000, 17.0156, 23.2500, 19.6250, 23.2812, 20.1406,\n",
      "        19.8906, 23.4062, 22.5312, 18.2344, 23.6406, 20.9688, 23.4844, 21.5312,\n",
      "        22.0156, 22.2188, 23.0781, 21.4375, 21.4375, 20.2031, 23.6406, 23.5156],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "Result for class : male\n",
      "Keyword                     Score    Acc.  Bias\n",
      "----------------------  ---------  ------  ------\n",
      "woman                    0.609375       1  M\n",
      "born                     0.484375       1  S\n",
      "young man                0.40625        1  S\n",
      "boy                      0.375          1  S\n",
      "student                  0.34375        1  S\n",
      "bullet                   0.265625       1  S\n",
      "undated                  0.078125       1  M\n",
      "living                  -0.125          1\n",
      "person                  -0.171875       1\n",
      "police                  -0.28125        1\n",
      "contestant              -0.296875       1\n",
      "arrested                -0.3125         1\n",
      "person to die           -0.34375        1\n",
      "face                    -0.359375       1\n",
      "house                   -0.359375       1\n",
      "head injury             -0.375          1\n",
      "genetic condition       -0.390625       1\n",
      "charged                 -0.40625        1\n",
      "rare                    -0.421875       1\n",
      "head                    -0.421875       1\n",
      "condition               -0.421875       1\n",
      "shown                   -0.453125       1\n",
      "car                     -0.453125       1\n",
      "man                     -0.5            1\n",
      "diagnosed               -0.515625       1\n",
      "named person            -0.5625         1\n",
      "fight                   -0.625          1\n",
      "shot                    -0.640625       1\n",
      "injury                  -0.65625        1\n",
      "genetic                 -0.71875        1\n",
      "person was found        -0.75           1\n",
      "rare genetic condition  -0.75           1\n",
      "found                   -0.765625       1\n",
      "found guilty            -0.78125        1\n",
      "bedroom                 -0.796875       1\n",
      "rare genetic            -0.8125         1\n",
      "murder                  -0.921875       1\n",
      "died                    -0.921875       1\n",
      "suspended               -1.14062        1\n",
      "beard                   -1.28125        1\n",
      "************************************************************\n"
     ]
    }
   ],
   "source": [
    "%cd /content/FACT/b2t\n",
    "print(\"fairfaces group 1 as minority, 3% like waterbirds. OUR METHOD\")\n",
    "\n",
    "\n",
    "# Define the Pipenv command and script\n",
    "command = \"python b2t.py --dataset fairfaces  --model best_model_Waterbirds_erm.pth --minority_group 1 --minority_ratio 0.03 --override_result --no_model\"\n",
    "# Run the command using the Pipenv environment\n",
    "!pipenv run bash -c \"{command}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 55321,
     "status": "ok",
     "timestamp": 1738351809736,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -60
    },
    "id": "5Mz7VA-W5saX",
    "outputId": "4bc0a4e9-b6d2-4de9-df96-60b59fc19a91"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/FACT/b2t\n",
      "fairfaces group 2 as minority, 3% like waterbirds. OUR METHOD\n",
      "Recommend running fairfaces initially with -1 as minority_ratio and extract_caption , allowing you to only have to extract_caption once! (all data will be captioned). Runs after this may use --override_result\n",
      "Classified result \"result/fairfaces_no_model.csv\" loaded\n",
      "100% 11/11 [00:08<00:00,  1.31it/s]\n",
      "Session will use filtered indices based on input arguments\n",
      "Start calculating scores..\n",
      "df_wrong_class_0[image] count: 1\n",
      "df_correct_class_0[image] count: 1460\n",
      "keywords_class_0 count: 40\n",
      "df_wrong_class_1[image] count: 724\n",
      "df_correct_class_1[image] count: 575\n",
      "keywords_class_1 count: 40\n",
      "Processing images: 100% 1/1 [00:00<00:00,  3.93it/s]\n",
      "Processing images: 100% 23/23 [00:06<00:00,  3.63it/s]\n",
      "Processing images: 100% 12/12 [00:02<00:00,  4.24it/s]\n",
      "Processing images: 100% 9/9 [00:02<00:00,  3.70it/s]\n",
      "similarity_wrong_class_0: tensor([24.2031, 21.6562, 24.0625, 23.1875, 23.1562, 22.7188, 19.4688, 22.3750,\n",
      "        23.5000, 20.3594, 23.0156, 20.0312, 23.1875, 23.2031, 24.2969, 22.6562,\n",
      "        20.0938, 24.5781, 24.1094, 22.0000, 20.7812, 18.9844, 21.8750, 21.1406,\n",
      "        23.4062, 23.5469, 20.1250, 23.1094, 23.0625, 23.6094, 22.4375, 21.2188,\n",
      "        19.1875, 22.8594, 21.7656, 20.5469, 23.4844, 22.7500, 22.1094, 23.0938],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "similarity_wrong_class_0: tensor([24.1094, 21.5000, 23.6250, 22.9688, 22.7656, 23.2656, 18.7812, 22.0000,\n",
      "        23.7031, 20.8906, 22.9688, 20.3750, 22.0625, 23.2344, 23.5156, 22.2188,\n",
      "        20.6406, 23.3906, 23.4375, 21.7188, 20.2344, 17.2969, 22.5625, 22.0781,\n",
      "        23.0312, 23.4531, 19.7812, 22.5469, 21.8594, 24.0000, 22.6094, 20.2344,\n",
      "        19.7812, 23.1250, 22.2656, 19.5938, 23.5938, 22.3438, 22.7812, 23.5156],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "similarity_wrong_class_0: tensor([24.2344, 21.7656, 23.7500, 24.0781, 22.0156, 23.8906, 23.9062, 23.0312,\n",
      "        22.5312, 22.7188, 19.9375, 22.8281, 24.0000, 22.4375, 21.3906, 23.2969,\n",
      "        21.4531, 22.0469, 22.5781, 23.7812, 18.5000, 20.5312, 17.3438, 22.7344,\n",
      "        18.6875, 21.2812, 23.8281, 22.4844, 22.0469, 23.3750, 19.9062, 22.6562,\n",
      "        22.5469, 24.3594, 23.2656, 19.8281, 23.1406, 22.6562, 21.7656, 23.6875],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "similarity_wrong_class_0: tensor([23.7188, 20.7031, 23.3906, 23.6250, 21.4375, 22.9531, 23.3750, 23.0000,\n",
      "        21.7969, 21.8594, 19.1875, 22.1094, 23.2969, 22.6875, 20.8750, 22.5156,\n",
      "        20.1875, 22.3594, 22.0000, 23.5000, 17.8906, 19.6250, 16.5469, 22.2500,\n",
      "        17.6562, 21.6406, 22.5938, 25.0938, 23.1562, 22.6562, 19.8281, 21.6406,\n",
      "        21.2969, 24.1562, 23.8281, 19.7031, 22.9375, 21.7656, 21.1719, 23.0469],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "Result for class : female\n",
      "Keyword                     Score    Acc.  Bias\n",
      "----------------------  ---------  ------  ------\n",
      "broken                   1.26562        1  S\n",
      "broken nose              1.25           1  S\n",
      "person was found         1.23438        1  S\n",
      "found                    1.0625         1  M\n",
      "bathroom                 1.03125        1  S\n",
      "nose                     1.01562        1  S\n",
      "rare genetic condition   0.9375         1  M\n",
      "shot                     0.90625        1  S\n",
      "missing                  0.890625       1  S\n",
      "rare genetic             0.859375       1  M\n",
      "bedroom                  0.796875       1  M\n",
      "genetic                  0.78125        1  M\n",
      "camera                   0.75           1  M\n",
      "diagnosed                0.734375       1  M\n",
      "smiles                   0.71875        1  S\n",
      "smile                    0.71875        1  S\n",
      "face                     0.703125       1  S\n",
      "named person             0.640625       1  M\n",
      "car                      0.609375       1  M\n",
      "neck                     0.59375        1  S\n",
      "head                     0.578125       1  M\n",
      "condition                0.578125       1  M\n",
      "genetic condition        0.53125        1  M\n",
      "person                   0.515625       1  M\n",
      "rare                     0.515625       1  M\n",
      "disease                  0.484375       1  S\n",
      "woman                    0.453125       1  S\n",
      "girl                     0.359375       1  S\n",
      "girl smiles              0.28125        1  S\n",
      "person smiles            0.203125       1  S\n",
      "mother                   0.203125       1  S\n",
      "tooth                    0.125          1  S\n",
      "undated                  0.078125       1  M\n",
      "student                  0.03125        1  M\n",
      "native                  -0.25           1\n",
      "born                    -0.3125         1\n",
      "boy                     -0.359375       1\n",
      "young girl              -0.5625         1\n",
      "black                   -1.10938        1\n",
      "black person            -2.60938        1\n"
     ]
    }
   ],
   "source": [
    "%cd /content/FACT/b2t\n",
    "print(\"fairfaces group 2 as minority, 3% like waterbirds. OUR METHOD\")\n",
    "\n",
    "\n",
    "# Define the Pipenv command and script\n",
    "command = \"python b2t.py --dataset fairfaces  --model best_model_Waterbirds_erm.pth --minority_group 2 --minority_ratio 0.03 --override_result --no_model\"\n",
    "# Run the command using the Pipenv environment\n",
    "!pipenv run bash -c \"{command}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 58009,
     "status": "ok",
     "timestamp": 1738351867735,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -60
    },
    "id": "s8TqBBVwdnfm",
    "outputId": "e10cf78b-bf43-48a8-f9e5-26538e18dfad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/FACT/b2t\n",
      "fairfaces group 3 as minority, 3% like waterbirds. OUR METHOD\n",
      "Recommend running fairfaces initially with -1 as minority_ratio and extract_caption , allowing you to only have to extract_caption once! (all data will be captioned). Runs after this may use --override_result\n",
      "Classified result \"result/fairfaces_no_model.csv\" loaded\n",
      "100% 12/12 [00:09<00:00,  1.26it/s]\n",
      "Session will use filtered indices based on input arguments\n",
      "Start calculating scores..\n",
      "df_wrong_class_0[image] count: 1\n",
      "df_correct_class_0[image] count: 1573\n",
      "keywords_class_0 count: 40\n",
      "df_wrong_class_1[image] count: 612\n",
      "df_correct_class_1[image] count: 787\n",
      "keywords_class_1 count: 40\n",
      "Processing images: 100% 1/1 [00:00<00:00,  4.01it/s]\n",
      "Processing images: 100% 25/25 [00:06<00:00,  3.61it/s]\n",
      "Processing images: 100% 10/10 [00:02<00:00,  3.96it/s]\n",
      "Processing images: 100% 13/13 [00:03<00:00,  4.17it/s]\n",
      "similarity_wrong_class_0: tensor([24.2031, 21.6562, 24.0625, 23.1875, 22.7188, 23.1562, 19.4688, 22.3750,\n",
      "        23.5000, 20.3594, 23.0156, 20.0312, 23.1875, 23.2031, 22.6562, 24.2969,\n",
      "        20.0938, 24.1094, 24.5781, 20.7812, 22.0000, 21.8750, 21.1406, 18.9844,\n",
      "        23.4062, 23.0625, 20.1250, 23.5469, 23.1094, 23.6094, 22.4375, 22.8594,\n",
      "        21.2188, 19.1875, 20.5469, 21.7656, 22.7500, 22.1094, 23.4844, 23.0938],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "similarity_wrong_class_0: tensor([24.1094, 21.5000, 23.6250, 22.9688, 23.2656, 22.7500, 18.7969, 22.0000,\n",
      "        23.7031, 20.8906, 22.9531, 20.3750, 22.0625, 23.2344, 22.2188, 23.5000,\n",
      "        20.6406, 23.4375, 23.3750, 20.2344, 21.7188, 22.5625, 22.0781, 17.2969,\n",
      "        23.0156, 21.8594, 19.7812, 23.4531, 22.5469, 23.9688, 22.5938, 23.1094,\n",
      "        20.2500, 19.7812, 19.5938, 22.2656, 22.3438, 22.7969, 23.5781, 23.5000],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "similarity_wrong_class_0: tensor([23.7344, 20.7031, 22.9375, 21.4531, 23.3906, 23.6406, 23.2969, 23.3594,\n",
      "        23.0000, 21.8438, 21.7969, 17.8750, 20.8594, 22.5000, 21.9844, 22.3438,\n",
      "        20.1875, 22.9219, 16.5469, 22.6875, 23.3594, 19.8281, 20.3906, 20.7969,\n",
      "        23.0469, 22.0938, 22.5781, 21.6719, 21.1719, 19.7812, 20.8438, 18.7656,\n",
      "        21.6406, 20.3594, 20.5938, 18.3906, 20.7500, 17.6562, 21.4844, 20.4062],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "similarity_wrong_class_0: tensor([24.2344, 21.7812, 23.8750, 22.0312, 23.7500, 24.1094, 24.0156, 23.9062,\n",
      "        23.0312, 22.7188, 22.5469, 18.5000, 21.3906, 23.2969, 22.6094, 22.0469,\n",
      "        21.4531, 23.5000, 17.3594, 22.4375, 23.5625, 19.9219, 21.4219, 21.9375,\n",
      "        23.6875, 22.8125, 23.7969, 22.6250, 21.7812, 20.5312, 21.6250, 19.5312,\n",
      "        22.6719, 20.9375, 21.7812, 19.6094, 21.7969, 18.6875, 22.3125, 20.6875],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "Result for class : female\n",
      "Keyword                     Score    Acc.  Bias\n",
      "----------------------  ---------  ------  ------\n",
      "born                     0.296875       1  S\n",
      "native                   0.25           1  S\n",
      "student                 -0.03125        1\n",
      "undated                 -0.09375        1\n",
      "contestant              -0.203125       1\n",
      "police                  -0.28125        1\n",
      "girl                    -0.359375       1\n",
      "woman                   -0.46875        1\n",
      "person                  -0.5            1\n",
      "rare                    -0.53125        1\n",
      "genetic condition       -0.546875       1\n",
      "condition               -0.578125       1\n",
      "degree                  -0.578125       1\n",
      "person to die           -0.578125       1\n",
      "found by police         -0.609375       1\n",
      "car                     -0.625          1\n",
      "head                    -0.625          1\n",
      "named person            -0.640625       1\n",
      "smiles                  -0.71875        1\n",
      "face                    -0.71875        1\n",
      "diagnosed               -0.75           1\n",
      "covered                 -0.75           1\n",
      "die                     -0.765625       1\n",
      "found guilty            -0.78125        1\n",
      "genetic                 -0.796875       1\n",
      "bedroom                 -0.8125         1\n",
      "image                   -0.828125       1\n",
      "rare genetic            -0.875          1\n",
      "rare genetic condition  -0.9375         1\n",
      "pictured                -0.953125       1\n",
      "hair                    -1.03125        1\n",
      "bathroom                -1.03125        1\n",
      "nose                    -1.03125        1\n",
      "murder                  -1.04688        1\n",
      "found                   -1.07812        1\n",
      "cancer                  -1.14062        1\n",
      "found unconscious       -1.1875         1\n",
      "car crash               -1.21875        1\n",
      "person was found        -1.21875        1\n",
      "broken                  -1.26562        1\n"
     ]
    }
   ],
   "source": [
    "%cd /content/FACT/b2t\n",
    "print(\"fairfaces group 3 as minority, 3% like waterbirds. OUR METHOD\")\n",
    "\n",
    "\n",
    "# Define the Pipenv command and script\n",
    "command = \"python b2t.py --dataset fairfaces  --model best_model_Waterbirds_erm.pth --minority_group 3 --minority_ratio 0.03 --override_result --no_model\"\n",
    "# Run the command using the Pipenv environment\n",
    "!pipenv run bash -c \"{command}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 66687,
     "status": "ok",
     "timestamp": 1738351934413,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -60
    },
    "id": "O3Fd8XuquudD",
    "outputId": "a61aded4-5a4b-4f02-b654-b95e788126fb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/FACT/b2t\n",
      "Waterbirds, train set because val not biased, OUR METHOD\n",
      "Start extracting captions...\n",
      "Processing Images: 100% 1199/1199 [00:23<00:00, 50.14image/s]\n",
      "Captions for 1199 images have been extracted.\n",
      "100% 5/5 [00:06<00:00,  1.27s/it]\n",
      "# of correct examples :  1199\n",
      "# of wrong examples :  0\n",
      "# of all examples :  1199\n",
      "Accuracy : 100.00 %\n",
      "Classified result stored\n",
      "Start calculating scores..\n",
      "df_wrong_class_0[image] count: 0\n",
      "df_correct_class_0[image] count: 933\n",
      "keywords_class_0 count: 40\n",
      "df_wrong_class_1[image] count: 0\n",
      "df_correct_class_1[image] count: 266\n",
      "keywords_class_1 count: 40\n",
      "Processing images: 100% 15/15 [00:07<00:00,  2.06it/s]\n",
      "Processing images: 100% 5/5 [00:02<00:00,  2.43it/s]\n",
      "similarity_wrong_class_0: 0\n",
      "similarity_wrong_class_0: tensor([22.7188, 24.2188, 22.4531, 26.5469, 19.4062, 18.5000, 20.3438, 20.6250,\n",
      "        26.4375, 19.6719, 25.0000, 25.1719, 20.0000, 24.3594, 20.6562, 22.9375,\n",
      "        20.7188, 18.5312, 24.9375, 19.2188, 20.3125, 22.6875, 26.0625, 19.8750,\n",
      "        21.1406, 26.7969, 17.9531, 25.8125, 19.4844, 22.1250, 18.8906, 18.7031,\n",
      "        18.5312, 20.3438, 23.2969, 22.8906, 22.6094, 25.8125, 17.1094, 20.0781],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "similarity_wrong_class_0: 0\n",
      "similarity_wrong_class_0: tensor([22.9531, 24.4062, 22.6562, 26.0625, 20.5000, 24.5312, 19.1562, 18.8281,\n",
      "        20.6406, 23.9375, 24.9219, 22.7500, 20.1875, 20.3125, 25.1875, 25.6562,\n",
      "        18.3750, 26.2812, 19.0625, 23.7344, 17.9688, 19.0469, 24.8750, 21.1250,\n",
      "        23.0625, 24.4062, 26.9375, 19.0938, 20.5469, 19.2656, 25.7812, 21.2031,\n",
      "        19.2969, 20.7031, 18.5469, 19.3125, 20.3438, 21.2344, 23.9531, 19.5156],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "Result for class : landbird\n",
      "Keyword                        Score    Acc.  Bias\n",
      "--------------------------  --------  ------  ------\n",
      "sunset                      -17.1094       1\n",
      "snow                        -17.9531       1\n",
      "beach                       -18.5          1\n",
      "garden                      -18.5312       1\n",
      "rocks                       -18.5312       1\n",
      "rock                        -18.7031       1\n",
      "sky                         -18.8906       1\n",
      "paradise                    -19.2188       1\n",
      "tree                        -19.4062       1\n",
      "lake                        -19.4844       1\n",
      "forest                      -19.6719       1\n",
      "rainforest                  -19.875        1\n",
      "water                       -20            1\n",
      "bamboo                      -20.0781       1\n",
      "tree branch                 -20.3125       1\n",
      "park                        -20.3438       1\n",
      "person                      -20.3438       1\n",
      "branch                      -20.625        1\n",
      "woods                       -20.6562       1\n",
      "pond                        -20.7188       1\n",
      "flies                       -21.1406       1\n",
      "seagull                     -22.125        1\n",
      "biological                  -22.4531       1\n",
      "male biological species     -22.6094       1\n",
      "wild                        -22.6875       1\n",
      "biological species          -22.7188       1\n",
      "flight                      -22.8906       1\n",
      "bird of paradise            -22.9375       1\n",
      "crow                        -23.2969       1\n",
      "species                     -24.2188       1\n",
      "prey                        -24.3594       1\n",
      "species in flight           -24.9375       1\n",
      "bird of prey                -25            1\n",
      "biological species perched  -25.1719       1\n",
      "bird flying                 -25.8125       1\n",
      "species perched             -25.8125       1\n",
      "bird flies                  -26.0625       1\n",
      "birds                       -26.4375       1\n",
      "bird                        -26.5469       1\n",
      "species of bird             -26.7969       1\n",
      "************************************************************\n",
      "Result for class : waterbird\n",
      "Keyword                Score    Acc.  Bias\n",
      "------------------  --------  ------  ------\n",
      "garden              -17.9688       1\n",
      "snow                -18.375        1\n",
      "ocean               -18.5469       1\n",
      "tree                -18.8281       1\n",
      "rocks               -19.0469       1\n",
      "forest              -19.0625       1\n",
      "sky                 -19.0938       1\n",
      "beach               -19.1562       1\n",
      "bamboo              -19.2656       1\n",
      "trees               -19.2969       1\n",
      "rainforest          -19.3125       1\n",
      "river               -19.5156       1\n",
      "woods               -20.1875       1\n",
      "flies               -20.3125       1\n",
      "branch              -20.3438       1\n",
      "water               -20.5          1\n",
      "lake                -20.5469       1\n",
      "person              -20.6406       1\n",
      "park                -20.7031       1\n",
      "pond                -21.125        1\n",
      "fish                -21.2031       1\n",
      "zoo                 -21.2344       1\n",
      "biological          -22.6562       1\n",
      "wild                -22.75         1\n",
      "biological species  -22.9531       1\n",
      "flight              -23.0625       1\n",
      "seagull flies       -23.7344       1\n",
      "prey                -23.9375       1\n",
      "swan                -23.9531       1\n",
      "flying              -24.4062       1\n",
      "species             -24.4062       1\n",
      "seagull             -24.5312       1\n",
      "duck                -24.875        1\n",
      "bird of prey        -24.9219       1\n",
      "species in flight   -25.1875       1\n",
      "bird flying         -25.6562       1\n",
      "bird flies          -25.7812       1\n",
      "bird                -26.0625       1\n",
      "birds               -26.2812       1\n",
      "species of bird     -26.9375       1\n"
     ]
    }
   ],
   "source": [
    "%cd /content/FACT/b2t\n",
    "# Define the Pipenv command and script\n",
    "print(\"Waterbirds, train set because val not biased, OUR METHOD\") # train just changed within code self, if want to run yourself please change\n",
    "\n",
    "\n",
    "command = \"python b2t.py --dataset waterbird --model best_model_Waterbirds_erm.pth --extract_caption --no_model\"\n",
    "#command = \"python b2t.py --profiler True\"\n",
    "\n",
    "# Run the command using the Pipenv environment\n",
    "!pipenv run bash -c \"{command}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 217301,
     "status": "ok",
     "timestamp": 1738353914065,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -60
    },
    "id": "4FN5_JWR-k6E",
    "outputId": "facf3842-a0f2-49dd-a93f-c8d0077677c5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/FACT/b2t\n",
      "CelebA, val set because val biased (0.9% for male blond), OUR METHOD\n",
      "100% 78/78 [01:01<00:00,  1.26it/s]\n",
      "# of correct examples :  19867\n",
      "# of wrong examples :  0\n",
      "# of all examples :  19867\n",
      "Accuracy : 100.00 %\n",
      "Classified result stored\n",
      "Start calculating scores..\n",
      "df_wrong_class_0[image] count: 0\n",
      "df_correct_class_0[image] count: 16811\n",
      "keywords_class_0 count: 40\n",
      "df_wrong_class_1[image] count: 0\n",
      "df_correct_class_1[image] count: 3056\n",
      "keywords_class_1 count: 40\n",
      "Processing images: 100% 263/263 [01:30<00:00,  2.92it/s]\n",
      "Processing images: 100% 48/48 [00:16<00:00,  2.91it/s]\n",
      "similarity_wrong_class_0: 0\n",
      "similarity_wrong_class_0: tensor([23.8125, 24.2188, 22.0469, 20.8438, 22.1250, 21.4531, 19.7656, 21.4219,\n",
      "        22.1406, 20.1094, 23.7812, 20.9062, 23.2656, 23.1250, 22.9375, 24.4688,\n",
      "        20.3438, 22.6562, 24.4219, 22.3906, 21.6250, 21.0312, 19.3438, 23.7812,\n",
      "        22.9688, 22.9531, 21.9844, 23.4375, 20.1719, 21.1562, 21.7656, 21.7188,\n",
      "        22.1406, 22.0469, 23.7188, 22.3906, 21.0312, 24.4531, 19.8750, 22.0938],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "similarity_wrong_class_0: 0\n",
      "similarity_wrong_class_0: tensor([22.9844, 23.9062, 22.2969, 22.2500, 21.3438, 20.8594, 20.1875, 23.0469,\n",
      "        22.0312, 21.3125, 25.2344, 23.0625, 20.5781, 23.1719, 22.5000, 19.3281,\n",
      "        19.8438, 24.4062, 21.0469, 19.8750, 20.6250, 20.4062, 18.7656, 21.2344,\n",
      "        22.1562, 23.9531, 23.5312, 24.4219, 23.4219, 24.1875, 21.5938, 22.0000,\n",
      "        22.3906, 20.2344, 24.8750, 24.3125, 23.8594, 22.0156, 21.2031, 22.9062],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "Result for class : not blond\n",
      "Keyword                   Score    Acc.  Bias\n",
      "--------------------  ---------  ------  ------\n",
      "face of person         2.32812        1  M\n",
      "contestant             2.3125         1  S\n",
      "actor as person        2.28125        1  S\n",
      "person                 2.07812        1  M\n",
      "actor                  1.67188        1  M\n",
      "named person           1.64062        1  S\n",
      "player                 1.64062        1  S\n",
      "tennis player          1.57812        1  S\n",
      "hair color             1.29688        1  M\n",
      "face                   1.125          1  M\n",
      "actress                0.984375       1  M\n",
      "style                  0.828125       1  M\n",
      "artist                 0.8125         1  S\n",
      "actor attends          0.796875       1  S\n",
      "model                  0.515625       1  M\n",
      "beauty                 0.25           1  S\n",
      "actor arrives          0.25           1  S\n",
      "man                    0              1  S\n",
      "fan                    0              1  S\n",
      "hair                  -0.015625       1\n",
      "favorite              -0.046875       1\n",
      "premiere              -0.09375        1\n",
      "clothing style        -0.09375        1\n",
      "talents               -0.15625        1\n",
      "football player       -0.375          1\n",
      "love her hair         -0.421875       1\n",
      "comedy                -0.515625       1\n",
      "thriller film         -0.6875         1\n",
      "comedy film           -0.71875        1\n",
      "model and actress     -0.984375       1\n",
      "color                 -1.10938        1\n",
      "premiere of romantic  -1.10938        1\n",
      "romantic comedy film  -1.23438        1\n",
      "film                  -1.29688        1\n",
      "premiere of comedy    -1.79688        1\n",
      "romantic comedy       -1.96875        1\n",
      "love                  -2.03125        1\n",
      "outfit                -2.26562        1\n",
      "premiere of thriller  -2.375          1\n",
      "thriller              -2.79688        1\n",
      "************************************************************\n",
      "Result for class : blond\n",
      "Keyword                   Score    Acc.  Bias\n",
      "--------------------  ---------  ------  ------\n",
      "actress                3.04688        1  M\n",
      "hair style             2.6875         1  S\n",
      "face of person         2.23438        1  M\n",
      "hair color             2.21875        1  M\n",
      "short hair             2.125          1  S\n",
      "short hairstyles       2              1  S\n",
      "hairstyle              1.76562        1  S\n",
      "person                 1.71875        1  M\n",
      "bob hairstyle          1.67188        1  S\n",
      "love her hair          1.34375        1  S\n",
      "hairstyles             1.23438        1  S\n",
      "face                   0.984375       1  M\n",
      "model                  0.875          1  M\n",
      "style                  0.859375       1  M\n",
      "actor                  0.796875       1  M\n",
      "love the hair          0.71875        1  S\n",
      "bob                    0.3125         1  S\n",
      "model and actress      0.203125       1  S\n",
      "hair                   0.109375       1  S\n",
      "premiere               0.0625         1  S\n",
      "favorite              -0.03125        1\n",
      "clothing style        -0.15625        1\n",
      "actor arrives         -0.171875       1\n",
      "short                 -0.1875         1\n",
      "premiere of romantic  -0.59375        1\n",
      "comedy film           -0.84375        1\n",
      "comedy                -0.875          1\n",
      "size                  -0.953125       1\n",
      "color                 -0.984375       1\n",
      "romantic comedy film  -1.14062        1\n",
      "film                  -1.32812        1\n",
      "clothing              -1.5625         1\n",
      "premiere of comedy    -1.60938        1\n",
      "weight                -1.78125        1\n",
      "romantic comedy       -1.95312        1\n",
      "love                  -2              1\n",
      "outfit                -2.3125         1\n",
      "feet size             -2.34375        1\n",
      "favorite outfit       -2.85938        1\n",
      "feet                  -3.42188        1\n"
     ]
    }
   ],
   "source": [
    "%cd /content/FACT/b2t\n",
    "# Define the Pipenv command and script\n",
    "print(\"CelebA, val set because val biased (0.9% for male blond), OUR METHOD\")\n",
    "command = \"python b2t.py --dataset celeba --model best_model_CelebA_erm.pth --extract_caption --no_model\"\n",
    "#command = \"python b2t.py --profiler True\"\n",
    "\n",
    "# Run the command using the Pipenv environment\n",
    "!pipenv run bash -c \"{command}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1653622,
     "status": "ok",
     "timestamp": 1738353696778,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -60
    },
    "id": "0Z6KYuqPygl6",
    "outputId": "7d6bfc16-ed19-496d-852a-e092467121d2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/FACT/b2t\n",
      "Imagenet, val set for bee, ant and boar OUR METHOD\n",
      "bee\n",
      "('n02206856', 309, 'bee')\n",
      "ant\n",
      "('n02219486', 310, 'ant, emmet, pismire')\n",
      "boar\n",
      "('n02396427', 342, 'wild boar, boar, Sus scrofa')\n",
      "Start extracting captions...\n",
      "Processing Images:  12% 6144/50000 [02:28<17:43, 41.22image/s]/root/.local/share/virtualenvs/FACT-Z5lTYn3d/lib/python3.8/site-packages/PIL/TiffImagePlugin.py:646: UserWarning: Metadata Warning, tag 33723 had too many entries: 4, expected 1\n",
      "  warnings.warn(\n",
      "Processing Images: 100% 50000/50000 [19:40<00:00, 42.34image/s]\n",
      "Captions for 50000 images have been extracted.\n",
      "100% 196/196 [07:10<00:00,  2.19s/it]\n",
      "# of correct examples :  50000\n",
      "# of wrong examples :  0\n",
      "# of all examples :  50000\n",
      "Accuracy : 100.00 %\n",
      "Classified result stored\n",
      "image_path: data/imagenet/data/imagenet-val/n02206856/\n",
      "Running scoring for bee\n",
      "progress at 0/3\n",
      "Start calculating scores..\n",
      "df_wrong_class_1[image] count: 0\n",
      "df_correct_class_1[image] count: 50\n",
      "keywords_class_1 count: 40\n",
      "got heere 11\n",
      "b2t.py:323: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_correct_class_1['updated_image'] = df_correct_class_1['image'].apply(\n",
      "Processing images: 100% 1/1 [00:01<00:00,  1.07s/it]\n",
      "got heere\n",
      "Result for class : bee\n",
      "Keyword            Score    Acc.  Bias\n",
      "--------------  --------  ------  ------\n",
      "road            -16.4688       1\n",
      "wire            -19.1406       1\n",
      "fence           -19.1562       1\n",
      "glass           -19.4375       1\n",
      "hand            -19.6875       1\n",
      "photo           -19.9062       1\n",
      "jar             -20.2188       1\n",
      "pink flower     -20.6562       1\n",
      "red flower      -20.8125       1\n",
      "white flower    -21.0469       1\n",
      "pink            -21.4062       1\n",
      "trapped         -21.6406       1\n",
      "purple flower   -21.7812       1\n",
      "plants          -22            1\n",
      "red             -22.0625       1\n",
      "blue flower     -22.0625       1\n",
      "purple          -22.1719       1\n",
      "piece           -22.2188       1\n",
      "wild            -22.4688       1\n",
      "sunflower       -22.625        1\n",
      "daisy           -22.7969       1\n",
      "garden          -22.8906       1\n",
      "biological      -23.25         1\n",
      "yellow flower   -23.6562       1\n",
      "close           -23.7188       1\n",
      "eats            -24            1\n",
      "flower          -24.25         1\n",
      "white           -24.6406       1\n",
      "yellow          -24.6875       1\n",
      "genus           -25.1094       1\n",
      "fly             -25.75         1\n",
      "flowering       -25.9062       1\n",
      "bee is trapped  -26.9688       1\n",
      "beehive         -27.625        1\n",
      "honey           -27.625        1\n",
      "bumblebee       -28.875        1\n",
      "honeybee        -29.4219       1\n",
      "bee             -29.6562       1\n",
      "honey bee       -29.7656       1\n",
      "bees            -29.9062       1\n",
      "image_path: data/imagenet/data/imagenet-val/n02219486/\n",
      "Running scoring for ant, emmet, pismire\n",
      "progress at 1/3\n",
      "Start calculating scores..\n",
      "df_wrong_class_1[image] count: 0\n",
      "df_correct_class_1[image] count: 50\n",
      "keywords_class_1 count: 40\n",
      "got heere 11\n",
      "b2t.py:323: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_correct_class_1['updated_image'] = df_correct_class_1['image'].apply(\n",
      "Processing images: 100% 1/1 [00:00<00:00,  1.71it/s]\n",
      "got heere\n",
      "Result for class : ant, emmet, pismire\n",
      "Keyword             Score    Acc.  Bias\n",
      "---------------  --------  ------  ------\n",
      "tree             -18.4219       1\n",
      "yellow flower    -18.4375       1\n",
      "green leaf       -18.8438       1\n",
      "rock             -19.2344       1\n",
      "red apple        -19.5625       1\n",
      "ripe red apple   -19.7344       1\n",
      "wall             -20.0781       1\n",
      "leaf             -20.4219       1\n",
      "flower           -20.875        1\n",
      "person found     -21.0625       1\n",
      "ripe red         -21.9375       1\n",
      "background       -22.0312       1\n",
      "green            -22.5156       1\n",
      "red              -22.6719       1\n",
      "black and white  -22.8594       1\n",
      "back             -23.4219       1\n",
      "close            -23.7812       1\n",
      "found            -23.8438       1\n",
      "mantis           -23.9688       1\n",
      "swarm            -24.1094       1\n",
      "bee              -24.25         1\n",
      "white            -24.3438       1\n",
      "mosquito         -24.4844       1\n",
      "tiny             -24.5469       1\n",
      "bees             -24.7656       1\n",
      "spider           -25.0469       1\n",
      "black            -25.2344       1\n",
      "small            -25.9375       1\n",
      "fly              -26.25         1\n",
      "beetle           -26.3594       1\n",
      "bug              -26.4062       1\n",
      "species          -27.125        1\n",
      "insect           -27.1875       1\n",
      "tiny insect      -27.3281       1\n",
      "wasp             -27.3281       1\n",
      "red ant          -27.9062       1\n",
      "insects          -27.9375       1\n",
      "ants             -28.7188       1\n",
      "ant              -29            1\n",
      "black ant        -29.375        1\n",
      "image_path: data/imagenet/data/imagenet-val/n02396427/\n",
      "Running scoring for wild boar, boar, Sus scrofa\n",
      "progress at 2/3\n",
      "Start calculating scores..\n",
      "df_wrong_class_1[image] count: 0\n",
      "df_correct_class_1[image] count: 50\n",
      "keywords_class_1 count: 40\n",
      "got heere 11\n",
      "b2t.py:323: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_correct_class_1['updated_image'] = df_correct_class_1['image'].apply(\n",
      "Processing images: 100% 1/1 [00:00<00:00,  1.81it/s]\n",
      "got heere\n",
      "Result for class : wild boar, boar, Sus scrofa\n",
      "Keyword                Score    Acc.  Bias\n",
      "------------------  --------  ------  ------\n",
      "snow                -17.2656       1\n",
      "pen                 -19.3125       1\n",
      "forest              -19.6406       1\n",
      "field               -20.0156       1\n",
      "grass               -20.4844       1\n",
      "wild ponies         -21.0156       1\n",
      "coyote              -21.2656       1\n",
      "ponies              -21.3906       1\n",
      "deer                -21.8906       1\n",
      "group               -22.4531       1\n",
      "wild                -22.7969       1\n",
      "mud                 -22.8281       1\n",
      "zoo                 -22.8594       1\n",
      "pair of wild        -22.9375       1\n",
      "black bear          -23.0312       1\n",
      "young               -23.0469       1\n",
      "young wild          -23.4531       1\n",
      "pair                -23.9062       1\n",
      "black               -24.1094       1\n",
      "bear                -24.1875       1\n",
      "mother              -24.25         1\n",
      "calf                -24.3594       1\n",
      "eating              -25.4219       1\n",
      "pigs eating grass   -26.9531       1\n",
      "wild boar sleeping  -27.5469       1\n",
      "pigs eating         -27.5938       1\n",
      "pig                 -27.9688       1\n",
      "young pig           -28.0625       1\n",
      "wild pigs           -28.2656       1\n",
      "wild pig            -28.4375       1\n",
      "pigs                -28.4688       1\n",
      "wild pigs eating    -28.5625       1\n",
      "wild boars          -28.6406       1\n",
      "wild boar           -28.8594       1\n",
      "boar sleeping       -29.1406       1\n",
      "wild boar eats      -29.4375       1\n",
      "young wild pig      -30.5156       1\n",
      "boar                -31.1094       1\n",
      "boar eats           -31.2656       1\n",
      "boars               -31.3125       1\n"
     ]
    }
   ],
   "source": [
    "%cd /content/FACT/b2t\n",
    "# Define the Pipenv command and script\n",
    "print(\"Imagenet, val set for bee, ant and boar OUR METHOD\")\n",
    "\n",
    "command = \"python b2t.py --dataset imagenet --model Resnet50 --no_model --class_names bee ant boar --extract_caption\"\n",
    "\n",
    "# Run the command using the Pipenv environment\n",
    "!pipenv run bash -c \"{command}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HYguQpD3D8XI"
   },
   "source": [
    "## Original B2T model ran on fairfaces, celeba and waterbird datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 232580,
     "status": "ok",
     "timestamp": 1738351056364,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -60
    },
    "id": "HaWeP7MTD8F4",
    "outputId": "49209114-ab75-42c2-d7d1-7b0408bd1382"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/FACT/b2t\n",
      "CelebA, val set because val biased (0.9% for male blond), NORMAL METHOD\n",
      "Pretrained model \"best_model_CelebA_erm.pth\" loaded\n",
      "100% 78/78 [01:24<00:00,  1.09s/it]\n",
      "# of correct examples :  18962\n",
      "# of wrong examples :  905\n",
      "# of all examples :  19867\n",
      "Accuracy : 95.44 %\n",
      "Classified result stored\n",
      "Start calculating scores..\n",
      "df_wrong_class_0[image] count: 476\n",
      "df_correct_class_0[image] count: 16335\n",
      "keywords_class_0 count: 40\n",
      "df_wrong_class_1[image] count: 429\n",
      "df_correct_class_1[image] count: 2627\n",
      "keywords_class_1 count: 40\n",
      "Processing images: 100% 8/8 [00:03<00:00,  2.34it/s]\n",
      "Processing images: 100% 256/256 [01:28<00:00,  2.90it/s]\n",
      "Processing images: 100% 7/7 [00:02<00:00,  3.13it/s]\n",
      "Processing images: 100% 42/42 [00:15<00:00,  2.72it/s]\n",
      "similarity_wrong_class_0: tensor([22.8281, 23.9688, 22.4375, 22.1562, 20.8438, 21.2969, 20.2812, 21.3594,\n",
      "        24.5156, 23.1719, 23.2344, 25.1250, 21.4844, 23.1406, 21.2969, 20.4062,\n",
      "        21.0000, 20.2500, 22.1562, 24.4688, 22.4844, 22.6094, 19.9531, 20.7188,\n",
      "        20.5000, 18.7812, 21.2344, 22.2031, 20.0156, 22.1406, 19.7812, 24.0469,\n",
      "        19.4375, 19.8594, 23.4688, 23.6250, 22.1250, 23.8750, 24.5000, 23.0000],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "similarity_wrong_class_0: tensor([23.8438, 24.2188, 22.1094, 22.0312, 20.8438, 21.4375, 20.1094, 21.6250,\n",
      "        23.4062, 22.6406, 23.2656, 23.0625, 21.0156, 22.9688, 21.0312, 20.3438,\n",
      "        20.9062, 20.1719, 21.7031, 24.4844, 22.9531, 22.9531, 19.8750, 20.6250,\n",
      "        20.5312, 18.8750, 21.2969, 22.0938, 20.3281, 22.0469, 19.9844, 23.5625,\n",
      "        19.0938, 19.8125, 22.9062, 21.6562, 22.2188, 23.5625, 24.4531, 22.4844],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "similarity_wrong_class_0: tensor([23.0781, 24.0469, 22.3125, 21.8125, 20.9531, 24.1250, 23.5938, 22.9062,\n",
      "        21.2812, 23.2031, 21.4531, 20.2344, 20.0625, 24.4062, 21.5469, 22.7031,\n",
      "        20.9688, 23.0156, 24.3125, 20.7969, 21.0000, 23.6094, 23.7656, 21.0625,\n",
      "        23.1094, 21.8594, 20.1250, 22.4375, 24.7031, 23.4062, 22.7656, 22.3125,\n",
      "        23.7969, 21.4219, 18.0156, 21.1719, 20.2344, 23.1562, 18.8750, 21.2344],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "similarity_wrong_class_0: tensor([22.9688, 23.8906, 22.2969, 22.3281, 20.8438, 25.4219, 23.1719, 23.0938,\n",
      "        21.3594, 23.1562, 21.2812, 20.1719, 20.6562, 24.4375, 22.5312, 22.5469,\n",
      "        19.7500, 23.0469, 24.4219, 21.0781, 21.7031, 24.3750, 23.6875, 21.0938,\n",
      "        22.8594, 21.7031, 20.2500, 22.0781, 24.9062, 23.5156, 22.4531, 22.4219,\n",
      "        23.9219, 20.8125, 17.9062, 20.7188, 19.7969, 23.4531, 18.3750, 20.8750],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "Result for class : not blond\n",
      "Keyword                   Score      Acc.  Bias\n",
      "--------------------  ---------  --------  ------\n",
      "actress                2.0625    0.963868  S\n",
      "love her hair          1.96875   0.966006  S\n",
      "hair color             1.10938   0.931034  S\n",
      "hairstyles             0.5625    0.948864  S\n",
      "model                  0.53125   0.965517  S\n",
      "love the hair          0.515625  0.952991  S\n",
      "hairstyle              0.484375  0.952941  S\n",
      "premiere of romantic   0.46875   0.950935  S\n",
      "long hair              0.453125  0.905263  S\n",
      "favorite outfit        0.34375   0.953333  S\n",
      "hair                   0.328125  0.942411  M\n",
      "length hair            0.3125    0.894737  S\n",
      "color                  0.265625  0.933481  S\n",
      "love                   0.171875  0.967456  M\n",
      "style                  0.171875  0.953216  S\n",
      "premiere               0.125     0.976428  S\n",
      "favorite               0.109375  0.953437  S\n",
      "romantic comedy film   0.09375   0.950935  S\n",
      "clothing               0.09375   0.953437  S\n",
      "clothing style         0.09375   0.953333  S\n",
      "romantic comedy        0.078125  0.950935  S\n",
      "outfit                 0.078125  0.953437  S\n",
      "premiere of comedy     0.0625    0.961872  S\n",
      "contestant             0.046875  0.964103  S\n",
      "feet size              0.046875  0.953333  S\n",
      "film                   0         0.977545  M\n",
      "face of person        -0.015625  0.966132\n",
      "face                  -0.03125   0.969622\n",
      "weight                -0.03125   0.953333\n",
      "size                  -0.0625    0.953333\n",
      "pop                   -0.09375   0.912371\n",
      "feet                  -0.09375   0.953333\n",
      "comedy film           -0.140625  0.950139\n",
      "long                  -0.203125  0.923358\n",
      "person                -0.25      0.975332\n",
      "comedy                -0.265625  0.957426\n",
      "romantic              -0.3125    0.950935\n",
      "artist                -0.34375   0.964587\n",
      "bob                   -0.46875   0.937838\n",
      "actor                 -1.01562   0.97534\n",
      "************************************************************\n",
      "Result for class : blond\n",
      "Keyword                   Score      Acc.  Bias\n",
      "--------------------  ---------  --------  ------\n",
      "man                    1.21875   0.382353  S\n",
      "football player        0.609375  0.266667  S\n",
      "football               0.5       0.25      S\n",
      "hard                   0.453125  0.666667  S\n",
      "romantic               0.4375    0.877358  S\n",
      "player                 0.421875  0.25641   S\n",
      "tennis                 0.359375  0         S\n",
      "native                 0.359375  0.8       S\n",
      "bob                    0.3125    0.945946  S\n",
      "portrait               0.25      0.802469  S\n",
      "comedy                 0.171875  0.881944  S\n",
      "artist                 0.15625   0.719298  S\n",
      "named                  0.15625   0.651163  S\n",
      "person                 0.15625   0.806744  S\n",
      "rock                   0.109375  0.666667  S\n",
      "actor                  0.109375  0.885176  S\n",
      "film                   0.109375  0.883838  S\n",
      "named person           0.078125  0.8       S\n",
      "love                   0.0625    0.902357  M\n",
      "face                   0.046875  0.868     S\n",
      "hair                   0.015625  0.905213  M\n",
      "face of person        -0.03125   0.866142\n",
      "movie                 -0.03125   0.719298\n",
      "style                 -0.03125   0.928082\n",
      "comedy film           -0.078125  0.903361\n",
      "singer                -0.109375  0.688889\n",
      "contestant            -0.109375  0.816092\n",
      "faces                 -0.109375  0.782609\n",
      "actor as person       -0.125     0.7\n",
      "romantic comedy       -0.125     0.877358\n",
      "model                 -0.1875    0.86\n",
      "hair style            -0.203125  0.76\n",
      "romantic comedy film  -0.28125   0.877358\n",
      "hairstyles            -0.296875  0.903226\n",
      "premiere              -0.515625  0.896842\n",
      "premiere of comedy    -0.59375   0.884615\n",
      "premiere of romantic  -0.703125  0.87619\n",
      "woman                 -0.765625  0.690909\n",
      "model and actress     -0.984375  0.851351\n",
      "actress               -1.29688   0.845188\n"
     ]
    }
   ],
   "source": [
    "%cd /content/FACT/b2t\n",
    "# Define the Pipenv command and script\n",
    "print(\"CelebA, val set because val biased (0.9% for male blond), NORMAL METHOD\")\n",
    "\n",
    "\n",
    "command = \"python b2t.py --dataset celeba --model best_model_CelebA_erm.pth\"\n",
    "#command = \"python b2t.py --profiler True\"\n",
    "\n",
    "# Run the command using the Pipenv environment\n",
    "!pipenv run bash -c \"{command}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 44629,
     "status": "ok",
     "timestamp": 1738351100973,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -60
    },
    "id": "oZHuH6hyEMcl",
    "outputId": "b14019e0-290c-468f-efde-12302bcff2a9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/FACT/b2t\n",
      "Waterbirds, train set because val not biased, NORMAL METHOD\n",
      "Classified result \"result/waterbird_best_model_Waterbirds_erm.csv\" loaded\n",
      "Start calculating scores..\n",
      "df_wrong_class_0[image] count: 100\n",
      "df_correct_class_0[image] count: 833\n",
      "keywords_class_0 count: 40\n",
      "df_wrong_class_1[image] count: 60\n",
      "df_correct_class_1[image] count: 206\n",
      "keywords_class_1 count: 40\n",
      "Processing images: 100% 2/2 [00:01<00:00,  1.99it/s]\n",
      "Processing images: 100% 14/14 [00:08<00:00,  1.68it/s]\n",
      "Processing images: 100% 1/1 [00:00<00:00,  2.04it/s]\n",
      "Processing images: 100% 4/4 [00:01<00:00,  2.04it/s]\n",
      "similarity_wrong_class_0: tensor([22.4062, 23.5938, 22.0156, 20.7969, 25.1406, 21.1250, 20.1875, 20.8125,\n",
      "        24.3281, 25.5000, 19.3125, 20.4219, 20.3750, 24.7188, 17.6875, 21.9219,\n",
      "        19.4531, 24.4062, 20.1719, 19.0156, 22.9844, 23.1250, 18.0469, 20.9844,\n",
      "        24.0156, 22.0312, 19.0625, 22.4844, 22.3438, 20.4375, 22.6406, 25.5156,\n",
      "        19.6406, 21.5312, 22.5781, 22.0781, 21.0312, 19.1406, 19.4219, 20.4062],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "similarity_wrong_class_0: tensor([22.7656, 24.2969, 22.5000, 18.2188, 26.7031, 19.8594, 20.3594, 21.1719,\n",
      "        21.8594, 26.1250, 18.8438, 19.3750, 20.1406, 26.4844, 17.3906, 22.1250,\n",
      "        18.4219, 25.0000, 20.7812, 20.8125, 24.0938, 22.8594, 17.0000, 21.1250,\n",
      "        21.1719, 19.0000, 16.5469, 22.6406, 21.4531, 21.2344, 21.8750, 26.5469,\n",
      "        19.1562, 19.0000, 23.3750, 19.5938, 19.0000, 18.7188, 20.4688, 17.2031],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "similarity_wrong_class_0: tensor([22.8750, 22.4531, 24.3594, 25.9688, 20.0938, 19.2500, 20.3125, 20.8125,\n",
      "        23.9531, 21.7969, 21.6406, 24.9062, 20.8438, 18.8906, 20.7500, 20.3906,\n",
      "        23.1250, 18.6250, 22.9375, 26.8281, 18.7500, 19.2188, 21.0312, 22.2969,\n",
      "        22.1875, 18.5938, 20.7812, 22.4688, 17.8125, 18.7969, 20.8438, 19.8281,\n",
      "        20.3594, 20.1562, 19.5625, 20.9062, 19.9219, 20.4062, 22.7031, 22.1875],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "similarity_wrong_class_0: tensor([22.9844, 22.7188, 24.4219, 26.0938, 18.4688, 17.5938, 20.7344, 18.5469,\n",
      "        23.9219, 19.7188, 18.5781, 24.9219, 18.8594, 19.2500, 18.8750, 16.3438,\n",
      "        21.7656, 19.1719, 22.7031, 26.9688, 19.1094, 19.6719, 18.9688, 22.7344,\n",
      "        22.3438, 18.1562, 21.0000, 22.4219, 18.5469, 19.1719, 20.6719, 18.8438,\n",
      "        19.5312, 19.4219, 20.1562, 21.3438, 19.9531, 20.7344, 23.1562, 22.2969],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "Result for class : landbird\n",
      "Keyword                 Score      Acc.  Bias\n",
      "------------------  ---------  --------  ------\n",
      "ocean                3.20312   0.666667  S\n",
      "sea                  3.03125   0.7       S\n",
      "seagull flies        2.84375   0.666667  S\n",
      "beach                2.57812   0.727273  S\n",
      "surfer               2.53125   0.777778  S\n",
      "wave                 2.51562   0.571429  S\n",
      "seal                 2.48438   0         S\n",
      "seagull              2.46875   0.6       S\n",
      "boat                 2.03125   0.818182  S\n",
      "water                1.26562   0.682927  S\n",
      "sunset               1.04688   0.75      S\n",
      "lake                 1.04688   0.777778  S\n",
      "rocks                1.03125   0.769231  S\n",
      "fish                 0.890625  0.727273  S\n",
      "kite                 0.765625  0.666667  S\n",
      "paradise             0.484375  0.954545  S\n",
      "sky                  0.46875   0.733333  S\n",
      "river                0.421875  0.6       S\n",
      "city                 0.296875  0.571429  S\n",
      "flight               0.265625  0.769231  S\n",
      "dog                  0.234375  0.333333  S\n",
      "morning             -0.140625  0.727273\n",
      "large               -0.15625   0.5\n",
      "person              -0.171875  0.811594\n",
      "tail                -0.203125  0\n",
      "biological species  -0.359375  0.939148\n",
      "flies               -0.359375  0.545455\n",
      "biological          -0.484375  0.939394\n",
      "species in flight   -0.59375   0.75\n",
      "pond                -0.609375  0.9\n",
      "bird flies          -0.625     0.714286\n",
      "species             -0.703125  0.939271\n",
      "yellow              -0.796875  0.666667\n",
      "crow                -0.796875  0.866667\n",
      "birds               -1.03125   0.958333\n",
      "sits                -1.04688   0.833333\n",
      "parrot              -1.10938   0.7\n",
      "bird                -1.5625    0.933099\n",
      "bird sits           -1.76562   0\n",
      "branch              -1.79688   0.95082\n",
      "************************************************************\n",
      "Result for class : waterbird\n",
      "Keyword                 Score      Acc.  Bias\n",
      "------------------  ---------  --------  ------\n",
      "bamboo forest        4.04688   0.2       S\n",
      "bamboo               3.0625    0.2       S\n",
      "forest               2.26562   0.333333  S\n",
      "woods                2.07812   0.636364  S\n",
      "jungle               2.0625    0.5       S\n",
      "rainforest           1.98438   0         S\n",
      "trees                1.875     0.333333  S\n",
      "garden               1.65625   0.125     S\n",
      "tree                 1.625     0.55      S\n",
      "bird of paradise     1.35938   0         S\n",
      "stump                0.984375  0.5       S\n",
      "grass                0.828125  0.5       S\n",
      "butterfly            0.734375  0         S\n",
      "wall                 0.4375    0.5       S\n",
      "wild                 0.234375  0.846154  S\n",
      "park                 0.171875  0.75      S\n",
      "selected             0.046875  0         S\n",
      "prey                 0.03125   0.764706  S\n",
      "bird of prey        -0.015625  0.75\n",
      "area                -0.03125   0\n",
      "species             -0.0625    0.730769\n",
      "biological species  -0.109375  0.730769\n",
      "found               -0.109375  0.666667\n",
      "bird                -0.125     0.6875\n",
      "species of bird     -0.140625  0.5\n",
      "digital             -0.15625   0\n",
      "art                 -0.21875   0\n",
      "biological          -0.265625  0.732824\n",
      "place               -0.328125  0\n",
      "rock                -0.359375  0.666667\n",
      "paradise            -0.359375  0\n",
      "sky                 -0.375     0.833333\n",
      "person              -0.421875  0.684211\n",
      "movie               -0.4375    0\n",
      "pictured            -0.4375    0.5\n",
      "photo               -0.453125  0.666667\n",
      "flight              -0.453125  0.857143\n",
      "rocks               -0.546875  0.714286\n",
      "actor               -0.59375   0\n",
      "snow                -0.734375  0.888889\n"
     ]
    }
   ],
   "source": [
    "%cd /content/FACT/b2t\n",
    "# Define the Pipenv command and script\n",
    "print(\"Waterbirds, train set because val not biased, NORMAL METHOD\") # train just changed within code self\n",
    "\n",
    "\n",
    "\n",
    "command = \"python b2t.py --dataset waterbird --model best_model_Waterbirds_erm.pth\"\n",
    "#command = \"python b2t.py --profiler True\"\n",
    "\n",
    "# Run the command using the Pipenv environment\n",
    "!pipenv run bash -c \"{command}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 152231,
     "status": "ok",
     "timestamp": 1738351253196,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -60
    },
    "id": "_QpLKq6lxjy5",
    "outputId": "c2e70151-8c30-41e5-8a5f-880f00abcca2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/FACT/b2t\n",
      "Recommend running fairfaces initially with -1 as minority_ratio and extract_caption , allowing you to only have to extract_caption once! (all data will be captioned). Runs after this may use --override_result\n",
      "Start extracting captions...\n",
      "Processing Images: 100% 3641/3641 [01:40<00:00, 36.36image/s]\n",
      "Captions for 3641 images have been extracted.\n",
      "Classified result \"result/fairfaces_best_model_Waterbirds_erm.csv\" loaded\n",
      "Start calculating scores..\n",
      "df_wrong_class_0[image] count: 1492\n",
      "df_correct_class_0[image] count: 429\n",
      "keywords_class_0 count: 40\n",
      "df_wrong_class_1[image] count: 632\n",
      "df_correct_class_1[image] count: 1088\n",
      "keywords_class_1 count: 40\n",
      "Processing images: 100% 24/24 [00:06<00:00,  3.85it/s]\n",
      "Processing images: 100% 7/7 [00:01<00:00,  4.01it/s]\n",
      "Processing images: 100% 10/10 [00:02<00:00,  4.07it/s]\n",
      "Processing images: 100% 17/17 [00:04<00:00,  4.03it/s]\n",
      "similarity_wrong_class_0: tensor([24.1719, 21.6562, 23.7031, 23.0781, 23.3125, 18.9219, 22.8281, 22.0938,\n",
      "        23.7969, 21.1094, 22.9375, 22.1875, 23.6406, 20.5312, 22.3594, 23.5625,\n",
      "        23.3438, 21.8125, 23.5156, 22.5469, 23.2031, 20.5938, 22.7344, 21.9844,\n",
      "        20.3906, 22.1875, 19.9219, 23.5625, 22.3594, 20.2344, 19.7969, 17.4062,\n",
      "        22.4375, 19.7656, 23.0781, 23.5938, 21.3906, 22.1875, 17.6094, 21.7969],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "similarity_wrong_class_0: tensor([23.9688, 20.9844, 23.3594, 22.6406, 23.1094, 18.4062, 22.5469, 21.6719,\n",
      "        23.3906, 20.1406, 22.9844, 21.6562, 23.1094, 19.8750, 21.7344, 22.8125,\n",
      "        22.8594, 21.4531, 23.2344, 22.6719, 22.4375, 20.9062, 21.9531, 21.4375,\n",
      "        19.7500, 21.6719, 19.3438, 23.1250, 21.9219, 20.2812, 19.7188, 16.9531,\n",
      "        21.9844, 18.9375, 23.1250, 23.2188, 20.4688, 21.8438, 17.2656, 21.1719],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "similarity_wrong_class_0: tensor([24.0156, 21.3438, 23.5000, 23.5469, 21.8438, 23.6719, 23.8438, 22.3594,\n",
      "        22.9531, 23.7188, 22.2344, 21.2344, 22.9219, 22.3125, 18.2656, 22.3906,\n",
      "        20.9688, 17.1250, 22.3281, 19.6250, 20.2031, 22.5938, 21.5781, 23.3281,\n",
      "        22.5625, 18.2656, 20.5469, 23.2500, 19.9844, 22.2344, 22.0156, 20.2656,\n",
      "        23.4531, 21.6562, 20.5312, 20.9844, 21.9688, 18.3125, 21.4375, 18.0938],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "similarity_wrong_class_0: tensor([24.0156, 21.2656, 23.4688, 23.6250, 21.7344, 23.6875, 23.9375, 22.3594,\n",
      "        23.0781, 23.6875, 22.1875, 21.1250, 23.0000, 22.1250, 18.2031, 22.2969,\n",
      "        20.8438, 16.8906, 22.6250, 19.5938, 20.0938, 22.5312, 21.4375, 23.1875,\n",
      "        22.5156, 18.2031, 20.5625, 23.2656, 19.8281, 22.2031, 22.0000, 20.1406,\n",
      "        23.7656, 21.3125, 20.1562, 20.9688, 21.9219, 18.2969, 21.1094, 18.0000],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "Result for class : male\n",
      "Keyword                     Score      Acc.  Bias\n",
      "----------------------  ---------  --------  ------\n",
      "beard                    0.96875   0.572368  S\n",
      "broken                   0.921875  0.394737  M\n",
      "suspended                0.828125  0.377778  S\n",
      "rare genetic             0.78125   0.480519  S\n",
      "genetic                  0.765625  0.480519  S\n",
      "rare genetic condition   0.75      0.480519  M\n",
      "found                    0.671875  0.481865  M\n",
      "shot                     0.65625   0.578947  M\n",
      "camera                   0.640625  0.313953  M\n",
      "died                     0.625     0.572816  S\n",
      "murder                   0.625     0.372093  S\n",
      "fight                    0.578125  0.457143  S\n",
      "found guilty             0.546875  0.391304  S\n",
      "diagnosed                0.53125   0.546218  M\n",
      "genetic condition        0.53125   0.480519  S\n",
      "car                      0.515625  0.474576  M\n",
      "smiles                   0.515625  0.324675  S\n",
      "person was found         0.484375  0.545455  S\n",
      "shown                    0.453125  0.467742  S\n",
      "bedroom                  0.453125  0.581395  M\n",
      "head injury              0.4375    0.486486  S\n",
      "named person             0.4375    0.4       S\n",
      "head                     0.4375    0.493827  M\n",
      "condition                0.421875  0.478873  M\n",
      "face                     0.40625   0.462025  M\n",
      "contestant               0.375     0.538462  S\n",
      "rare                     0.359375  0.45122   M\n",
      "man                      0.34375   0.539568  M\n",
      "charged                  0.34375   0.404255  S\n",
      "house                    0.34375   0.59375   S\n",
      "person to die            0.28125   0.516129  M\n",
      "arrested                 0.28125   0.437186  S\n",
      "boy                      0.203125  0.30198   M\n",
      "person                   0.203125  0.491587  S\n",
      "bullet                   0.078125  0.412698  S\n",
      "student                 -0.046875  0.459259\n",
      "undated                 -0.046875  0.486111\n",
      "young man               -0.046875  0.52381\n",
      "born                    -0.125     0.506024\n",
      "woman                   -0.3125    0.444444\n",
      "************************************************************\n"
     ]
    }
   ],
   "source": [
    "# first run  this to get all captions for fairfaces, ignore output\n",
    "\n",
    "%cd /content/FACT/b2t\n",
    "# Define the Pipenv command and script\n",
    "command = \"python b2t.py --dataset fairfaces  --model best_model_Waterbirds_erm.pth --minority_group 0 --minority_ratio -1 --extract_caption\"\n",
    "# Run the command using the Pipenv environment\n",
    "!pipenv run bash -c \"{command}\"\n",
    "\n",
    "#, then run following cells with subset minorities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 55121,
     "status": "ok",
     "timestamp": 1738351308305,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -60
    },
    "id": "iezKtakMQQFV",
    "outputId": "46412e6d-03e5-4cc9-84d7-a2e00ac175f4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/FACT/b2t\n",
      "fairfaces group 0 as minority, 3% like waterbirds. NORMAL METHOD\n",
      "Recommend running fairfaces initially with -1 as minority_ratio and extract_caption , allowing you to only have to extract_caption once! (all data will be captioned). Runs after this may use --override_result\n",
      "Classified result \"result/fairfaces_best_model_Waterbirds_erm.csv\" loaded\n",
      "100% 11/11 [00:09<00:00,  1.21it/s]\n",
      "Session will use filtered indices based on input arguments\n",
      "Start calculating scores..\n",
      "df_wrong_class_0[image] count: 1055\n",
      "df_correct_class_0[image] count: 317\n",
      "keywords_class_0 count: 40\n",
      "df_wrong_class_1[image] count: 466\n",
      "df_correct_class_1[image] count: 758\n",
      "keywords_class_1 count: 40\n",
      "Processing images: 100% 17/17 [00:04<00:00,  3.53it/s]\n",
      "Processing images: 100% 5/5 [00:01<00:00,  3.97it/s]\n",
      "Processing images: 100% 8/8 [00:01<00:00,  4.26it/s]\n",
      "Processing images: 100% 12/12 [00:02<00:00,  4.02it/s]\n",
      "similarity_wrong_class_0: tensor([24.1719, 21.6719, 23.7188, 23.0625, 23.3125, 22.8281, 18.8906, 22.0938,\n",
      "        23.7969, 21.1406, 22.9688, 22.1875, 23.6406, 20.5156, 22.3750, 21.8125,\n",
      "        23.5625, 20.5625, 23.5156, 23.5625, 21.9844, 23.2188, 19.9219, 22.2188,\n",
      "        22.5312, 20.3906, 23.3594, 22.7500, 19.7969, 22.3438, 20.2188, 22.4531,\n",
      "        22.1719, 19.7969, 17.4219, 21.7969, 21.3906, 23.6094, 23.1250, 21.0938],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "similarity_wrong_class_0: tensor([23.9531, 20.9688, 23.3750, 22.6250, 23.0781, 22.6094, 18.4219, 21.6406,\n",
      "        23.3750, 20.1406, 22.9219, 21.6562, 23.0625, 19.8906, 21.7188, 21.4531,\n",
      "        22.7969, 20.8594, 23.2188, 23.1094, 21.4688, 22.4062, 19.3750, 21.5938,\n",
      "        22.6406, 19.7344, 22.8438, 21.9375, 19.7031, 21.9375, 20.2969, 21.9688,\n",
      "        21.8594, 18.9531, 16.9844, 21.2031, 20.4688, 23.2031, 23.1250, 20.7656],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "similarity_wrong_class_0: tensor([24.0469, 21.3750, 23.5000, 23.5938, 21.8594, 23.6875, 22.3594, 23.8281,\n",
      "        23.0312, 21.2500, 22.2656, 22.9375, 23.7344, 22.3281, 22.3906, 18.2812,\n",
      "        20.9844, 17.1719, 20.2188, 19.6562, 22.3438, 22.6250, 18.2812, 22.5625,\n",
      "        21.6250, 23.2969, 20.5625, 23.3594, 22.2344, 22.0312, 20.0000, 23.4844,\n",
      "        20.2969, 20.5625, 21.7344, 23.0938, 18.1406, 20.9844, 18.3281, 20.6875],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "similarity_wrong_class_0: tensor([23.9844, 21.2500, 23.4688, 23.5781, 21.6875, 23.6719, 22.3438, 23.9062,\n",
      "        23.0156, 21.1094, 22.1719, 22.9688, 23.6562, 22.0938, 22.2656, 18.1875,\n",
      "        20.8281, 16.8906, 20.0625, 19.5781, 22.6094, 22.5000, 18.2031, 22.5000,\n",
      "        21.4531, 23.2812, 20.5625, 23.1719, 22.1875, 21.9844, 19.7812, 23.7656,\n",
      "        20.1094, 20.1250, 21.2656, 23.0156, 17.9844, 20.9531, 18.2969, 20.6406],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "Result for class : male\n",
      "Keyword                     Score      Acc.  Bias\n",
      "----------------------  ---------  --------  ------\n",
      "beard                    1         0.598214  S\n",
      "broken                   0.921875  0.333333  M\n",
      "suspended                0.84375   0.34375   S\n",
      "rare genetic             0.8125    0.42      M\n",
      "genetic                  0.8125    0.42      S\n",
      "rare genetic condition   0.765625  0.42      M\n",
      "found                    0.703125  0.52381   M\n",
      "camera                   0.65625   0.315789  M\n",
      "died                     0.65625   0.605634  S\n",
      "shot                     0.625     0.602151  M\n",
      "smiles                   0.625     0.327273  S\n",
      "murder                   0.59375   0.342857  S\n",
      "genetic condition        0.578125  0.42      M\n",
      "fight                    0.546875  0.490909  S\n",
      "diagnosed                0.53125   0.530864  M\n",
      "found guilty             0.515625  0.352941  S\n",
      "person was found         0.515625  0.714286  M\n",
      "shown                    0.484375  0.475     S\n",
      "car                      0.46875   0.507812  M\n",
      "named person             0.453125  0.444444  S\n",
      "condition                0.453125  0.46875   M\n",
      "head                     0.4375    0.502857  M\n",
      "bedroom                  0.4375    0.677966  M\n",
      "face                     0.421875  0.487395  M\n",
      "contestant               0.40625   0.514286  S\n",
      "head injury              0.40625   0.6       S\n",
      "rare                     0.359375  0.388889  M\n",
      "man                      0.34375   0.562092  M\n",
      "degree                   0.328125  0.354839  M\n",
      "charged                  0.3125    0.421053  S\n",
      "person to die            0.296875  0.590909  M\n",
      "boy                      0.234375  0.282759  M\n",
      "arrested                 0.21875   0.42953   S\n",
      "person                   0.21875   0.500511  M\n",
      "bullet                   0.09375   0.391304  S\n",
      "student                  0.046875  0.460784  M\n",
      "young man                0         0.533333  S\n",
      "undated                 -0.078125  0.478261\n",
      "born                    -0.109375  0.45283\n",
      "woman                   -0.296875  0.453125\n",
      "************************************************************\n"
     ]
    }
   ],
   "source": [
    "%cd /content/FACT/b2t\n",
    "print(\"fairfaces group 0 as minority, 3% like waterbirds. NORMAL METHOD\")\n",
    "\n",
    "\n",
    "# Define the Pipenv command and script\n",
    "command = \"python b2t.py --dataset fairfaces  --model best_model_Waterbirds_erm.pth --minority_group 0 --minority_ratio 0.03 --override_result\"\n",
    "# Run the command using the Pipenv environment\n",
    "!pipenv run bash -c \"{command}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 58033,
     "status": "ok",
     "timestamp": 1738351366328,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -60
    },
    "id": "910PDcEcQY08",
    "outputId": "4987ff7e-cfee-4c11-c2b1-2ac7b025f0fd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/FACT/b2t\n",
      "fairfaces group 1 as minority, 3% like waterbirds. NORMAL METHOD\n",
      "Recommend running fairfaces initially with -1 as minority_ratio and extract_caption , allowing you to only have to extract_caption once! (all data will be captioned). Runs after this may use --override_result\n",
      "Classified result \"result/fairfaces_best_model_Waterbirds_erm.csv\" loaded\n",
      "100% 12/12 [00:10<00:00,  1.19it/s]\n",
      "Session will use filtered indices based on input arguments\n",
      "Start calculating scores..\n",
      "df_wrong_class_0[image] count: 1139\n",
      "df_correct_class_0[image] count: 413\n",
      "keywords_class_0 count: 40\n",
      "df_wrong_class_1[image] count: 517\n",
      "df_correct_class_1[image] count: 860\n",
      "keywords_class_1 count: 40\n",
      "Processing images: 100% 18/18 [00:05<00:00,  3.59it/s]\n",
      "Processing images: 100% 7/7 [00:01<00:00,  4.23it/s]\n",
      "Processing images: 100% 9/9 [00:02<00:00,  3.87it/s]\n",
      "Processing images: 100% 14/14 [00:03<00:00,  4.05it/s]\n",
      "similarity_wrong_class_0: tensor([24.1094, 21.4062, 23.5312, 22.9219, 23.3906, 22.7344, 18.7500, 23.0625,\n",
      "        23.6719, 20.2969, 21.9531, 20.6562, 23.1406, 20.1719, 22.0156, 22.1094,\n",
      "        20.7500, 23.5000, 22.0781, 21.6875, 17.1562, 22.6562, 23.3281, 22.9688,\n",
      "        23.3906, 22.6406, 22.4688, 19.7031, 24.4062, 22.9844, 21.7500, 20.2344,\n",
      "        23.3906, 23.4844, 19.8750, 23.5781, 23.7500, 21.0781, 19.4219, 21.5312],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "similarity_wrong_class_0: tensor([24.1562, 21.8125, 23.8906, 23.0781, 22.9219, 22.8281, 18.9062, 22.6562,\n",
      "        23.8125, 20.6094, 22.1406, 21.5781, 23.5000, 20.4219, 22.1875, 22.0156,\n",
      "        20.3750, 23.5312, 22.6094, 21.8594, 17.6875, 22.3281, 23.5625, 23.2031,\n",
      "        23.5781, 22.4844, 22.7812, 20.0000, 22.7812, 22.2656, 22.1875, 20.2812,\n",
      "        23.6406, 22.8906, 19.5469, 23.5938, 24.0469, 21.5781, 20.0938, 22.0156],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "similarity_wrong_class_0: tensor([24.0312, 21.3750, 23.5000, 23.5781, 21.8594, 23.6719, 22.3438, 23.8281,\n",
      "        23.0000, 22.2656, 21.2344, 23.7188, 22.9219, 22.3125, 22.3906, 18.2812,\n",
      "        21.0000, 17.1719, 19.6406, 20.2188, 22.3125, 22.5625, 22.6094, 21.6250,\n",
      "        18.2969, 20.5469, 23.2812, 23.3438, 22.2344, 20.2969, 20.0000, 20.5312,\n",
      "        22.0312, 21.3750, 23.4531, 21.6875, 18.3125, 23.0938, 18.1250, 17.0938],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "similarity_wrong_class_0: tensor([24.0156, 21.2656, 23.4531, 23.6094, 21.7344, 23.6719, 22.3438, 23.9531,\n",
      "        23.0469, 22.1875, 21.1250, 23.6875, 22.9844, 22.1094, 22.2969, 18.2031,\n",
      "        20.8438, 16.9219, 19.6094, 20.0938, 22.6094, 22.5156, 22.5156, 21.4688,\n",
      "        18.2031, 20.5781, 23.2812, 23.2031, 22.2188, 20.1406, 19.8125, 20.1719,\n",
      "        22.0000, 21.2344, 23.7656, 21.2969, 18.3125, 23.0781, 18.0156, 17.0000],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "Result for class : male\n",
      "Keyword                     Score      Acc.  Bias\n",
      "----------------------  ---------  --------  ------\n",
      "black person             1.625     0.625     S\n",
      "child                    0.71875   0.511111  S\n",
      "boy smiles               0.59375   0.3125    S\n",
      "boy                      0.46875   0.30303   M\n",
      "student                  0.40625   0.469027  S\n",
      "woman                    0.375     0.453333  S\n",
      "born                     0.328125  0.451613  M\n",
      "bullet                   0.328125  0.387755  S\n",
      "person was arrested      0.15625   0.666667  S\n",
      "smiles                   0.09375   0.322581  S\n",
      "player                  -0.015625  0.555556\n",
      "genetic condition       -0.03125   0.431034\n",
      "person                  -0.046875  0.5\n",
      "undated                 -0.046875  0.5\n",
      "arrested                -0.09375   0.426829\n",
      "face                    -0.140625  0.484615\n",
      "head                    -0.15625   0.492063\n",
      "car                     -0.15625   0.489362\n",
      "diagnosed               -0.171875  0.543478\n",
      "rare                    -0.171875  0.403226\n",
      "person to die           -0.1875    0.583333\n",
      "condition               -0.1875    0.477876\n",
      "genetic                 -0.234375  0.431034\n",
      "rare genetic condition  -0.234375  0.431034\n",
      "camera                  -0.25      0.328571\n",
      "named person            -0.25      0.444444\n",
      "fight                   -0.296875  0.483333\n",
      "allegedly               -0.296875  0.405405\n",
      "rare genetic            -0.3125    0.431034\n",
      "shot                    -0.3125    0.571429\n",
      "man                     -0.359375  0.557522\n",
      "person was found        -0.359375  0.615385\n",
      "found                   -0.40625   0.508143\n",
      "found guilty            -0.4375    0.368421\n",
      "murder                  -0.484375  0.368421\n",
      "punched                 -0.5       0.388889\n",
      "died                    -0.53125   0.597561\n",
      "bedroom                 -0.53125   0.666667\n",
      "suspended               -0.671875  0.375\n",
      "beard                   -0.921875  0.587302\n",
      "************************************************************\n"
     ]
    }
   ],
   "source": [
    "%cd /content/FACT/b2t\n",
    "print(\"fairfaces group 1 as minority, 3% like waterbirds. NORMAL METHOD\")\n",
    "\n",
    "\n",
    "# Define the Pipenv command and script\n",
    "command = \"python b2t.py --dataset fairfaces  --model best_model_Waterbirds_erm.pth --minority_group 1 --minority_ratio 0.03 --override_result\"\n",
    "# Run the command using the Pipenv environment\n",
    "!pipenv run bash -c \"{command}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 55215,
     "status": "ok",
     "timestamp": 1738351421531,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -60
    },
    "id": "u26uXpy7QobZ",
    "outputId": "404eb6ff-2c53-4641-d496-bba1e28c089c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/FACT/b2t\n",
      "fairfaces group 2 as minority, 3% like waterbirds. NORMAL METHOD\n",
      "Recommend running fairfaces initially with -1 as minority_ratio and extract_caption , allowing you to only have to extract_caption once! (all data will be captioned). Runs after this may use --override_result\n",
      "Classified result \"result/fairfaces_best_model_Waterbirds_erm.csv\" loaded\n",
      "100% 11/11 [00:09<00:00,  1.15it/s]\n",
      "Session will use filtered indices based on input arguments\n",
      "Start calculating scores..\n",
      "df_wrong_class_0[image] count: 739\n",
      "df_correct_class_0[image] count: 722\n",
      "keywords_class_0 count: 40\n",
      "df_wrong_class_1[image] count: 964\n",
      "df_correct_class_1[image] count: 335\n",
      "keywords_class_1 count: 40\n",
      "Processing images: 100% 12/12 [00:03<00:00,  3.49it/s]\n",
      "Processing images: 100% 12/12 [00:02<00:00,  4.21it/s]\n",
      "Processing images: 100% 16/16 [00:04<00:00,  3.52it/s]\n",
      "Processing images: 100% 6/6 [00:01<00:00,  4.35it/s]\n",
      "similarity_wrong_class_0: tensor([24.1719, 21.5938, 23.6250, 23.5312, 23.0625, 22.8125, 18.9062, 22.0781,\n",
      "        23.7969, 23.1406, 20.9219, 20.4844, 22.2031, 20.3750, 23.7031, 22.3438,\n",
      "        21.7969, 20.6719, 23.5781, 23.2344, 22.6562, 22.7344, 22.2500, 19.9219,\n",
      "        19.8750, 19.6406, 21.9062, 23.5312, 23.4844, 20.2031, 21.2656, 21.7031,\n",
      "        23.6562, 23.8594, 22.1406, 17.2656, 23.2969, 22.4688, 21.3594, 21.0469],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "similarity_wrong_class_0: tensor([24.0625, 21.4062, 23.6406, 22.9844, 22.8594, 22.7188, 18.6562, 21.9062,\n",
      "        23.6094, 22.7656, 20.8750, 20.2500, 21.9375, 20.0938, 23.3125, 21.8281,\n",
      "        21.6406, 20.6094, 23.1875, 22.8281, 22.4688, 22.3594, 22.1719, 19.6250,\n",
      "        19.6875, 19.5469, 21.8281, 23.3750, 23.4062, 20.2812, 21.0781, 21.6250,\n",
      "        22.9688, 23.7969, 22.0469, 17.3438, 23.1875, 22.2344, 21.0312, 20.9844],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "similarity_wrong_class_0: tensor([24.1406, 21.5469, 23.6875, 21.9062, 23.7969, 23.6719, 23.9844, 22.5469,\n",
      "        23.0312, 23.8594, 22.3750, 21.3125, 23.1250, 18.3750, 22.1562, 22.4844,\n",
      "        21.1875, 17.2031, 23.4062, 22.5156, 22.6406, 23.5469, 19.9062, 19.7969,\n",
      "        21.2031, 23.5156, 21.6719, 18.4844, 22.6406, 23.5469, 21.4531, 22.4531,\n",
      "        20.6406, 20.3594, 21.6562, 22.2812, 20.3750, 22.1250, 21.5469, 19.3906],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "similarity_wrong_class_0: tensor([23.6406, 20.5781, 22.8594, 21.3125, 23.3125, 23.3594, 23.6094, 21.7812,\n",
      "        22.9844, 23.1875, 21.7031, 20.7500, 22.4531, 17.7812, 22.2656, 21.8906,\n",
      "        20.0312, 16.3750, 22.7969, 22.6250, 22.1250, 22.5000, 19.7656, 19.0938,\n",
      "        20.2969, 23.3438, 21.0625, 17.5312, 22.1719, 23.0000, 20.7812, 21.5000,\n",
      "        20.3594, 19.4688, 20.7500, 21.1719, 19.6406, 21.3750, 20.3438, 18.6250],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "Result for class : female\n",
      "Keyword                     Score      Acc.  Bias\n",
      "----------------------  ---------  --------  ------\n",
      "found unconscious        1.20312   0.545455  S\n",
      "broken                   1.15625   0.589041  M\n",
      "broken nose              1.10938   0.611111  S\n",
      "person was found         1.04688   0.6       M\n",
      "found                    0.96875   0.59      M\n",
      "bathroom                 0.953125  0.586957  S\n",
      "nose                     0.953125  0.578947  S\n",
      "hair                     0.90625   0.717391  S\n",
      "cancer                   0.90625   0.71875   S\n",
      "shot                     0.890625  0.5       M\n",
      "rare genetic condition   0.828125  0.567308  M\n",
      "bedroom                  0.828125  0.607595  S\n",
      "die                      0.765625  0.692308  S\n",
      "rare genetic             0.765625  0.567308  M\n",
      "image                    0.75      0.65625   S\n",
      "covered                  0.734375  0.515152  S\n",
      "camera                   0.703125  0.623188  M\n",
      "found guilty             0.671875  0.428571  M\n",
      "face                     0.671875  0.614815  M\n",
      "diagnosed                0.671875  0.635659  M\n",
      "genetic                  0.671875  0.567308  M\n",
      "found by police          0.609375  0.428571  S\n",
      "person to die            0.609375  0.692308  M\n",
      "head                     0.59375   0.58      M\n",
      "condition                0.59375   0.566176  M\n",
      "car                      0.59375   0.594059  M\n",
      "rare                     0.5625    0.558559  M\n",
      "named person             0.546875  0.875     M\n",
      "smiles                   0.515625  0.666667  M\n",
      "person                   0.5       0.618221  M\n",
      "genetic condition        0.484375  0.567308  M\n",
      "disease                  0.46875   0.547619  S\n",
      "woman                    0.375     0.675393  M\n",
      "girl                     0.3125    0.553922  S\n",
      "police                   0.28125   0.4375    S\n",
      "contestant               0.171875  0.684211  S\n",
      "undated                  0.140625  0.692308  S\n",
      "student                  0.046875  0.605839  M\n",
      "native                  -0.109375  0.753086\n",
      "born                    -0.109375  0.541667\n"
     ]
    }
   ],
   "source": [
    "%cd /content/FACT/b2t\n",
    "print(\"fairfaces group 2 as minority, 3% like waterbirds. NORMAL METHOD\")\n",
    "\n",
    "\n",
    "# Define the Pipenv command and script\n",
    "command = \"python b2t.py --dataset fairfaces  --model best_model_Waterbirds_erm.pth --minority_group 2 --minority_ratio 0.03 --override_result\"\n",
    "# Run the command using the Pipenv environment\n",
    "!pipenv run bash -c \"{command}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 56585,
     "status": "ok",
     "timestamp": 1738351478110,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -60
    },
    "id": "9zUJU9LXQtxC",
    "outputId": "aa6e2619-c07e-4bb4-890f-d354ebb2457b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/FACT/b2t\n",
      "fairfaces group 3 as minority, 3% like waterbirds. NORMAL METHOD\n",
      "Recommend running fairfaces initially with -1 as minority_ratio and extract_caption , allowing you to only have to extract_caption once! (all data will be captioned). Runs after this may use --override_result\n",
      "Classified result \"result/fairfaces_best_model_Waterbirds_erm.csv\" loaded\n",
      "100% 12/12 [00:10<00:00,  1.18it/s]\n",
      "Session will use filtered indices based on input arguments\n",
      "Start calculating scores..\n",
      "df_wrong_class_0[image] count: 801\n",
      "df_correct_class_0[image] count: 773\n",
      "keywords_class_0 count: 40\n",
      "df_wrong_class_1[image] count: 879\n",
      "df_correct_class_1[image] count: 520\n",
      "keywords_class_1 count: 40\n",
      "Processing images: 100% 13/13 [00:03<00:00,  3.33it/s]\n",
      "Processing images: 100% 13/13 [00:03<00:00,  4.24it/s]\n",
      "Processing images: 100% 14/14 [00:04<00:00,  3.47it/s]\n",
      "Processing images: 100% 9/9 [00:02<00:00,  4.31it/s]\n",
      "similarity_wrong_class_0: tensor([24.1562, 21.5781, 23.6094, 23.5156, 23.0469, 22.7969, 18.9062, 22.0625,\n",
      "        23.7812, 23.1250, 20.8594, 20.4688, 20.3594, 22.1719, 22.3125, 23.6875,\n",
      "        20.6719, 21.7812, 23.5469, 22.6406, 23.2031, 22.2344, 21.8906, 22.7031,\n",
      "        23.2656, 19.8594, 19.6250, 19.9219, 23.4688, 23.5156, 23.6250, 20.2031,\n",
      "        21.2500, 21.6719, 17.2500, 22.4375, 23.8438, 22.1250, 21.3438, 23.0781],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "similarity_wrong_class_0: tensor([24.0625, 21.4375, 23.6562, 23.0000, 22.8750, 22.7188, 18.6719, 21.9219,\n",
      "        23.6094, 22.7969, 20.9062, 20.2656, 20.1094, 21.9375, 21.8281, 23.3125,\n",
      "        20.6094, 21.6562, 23.2031, 22.4688, 22.8281, 22.2031, 21.8281, 22.3750,\n",
      "        23.1875, 19.7031, 19.5625, 19.6250, 23.4062, 23.3906, 22.9844, 20.2969,\n",
      "        21.0938, 21.6250, 17.3438, 22.2500, 23.7969, 22.0469, 21.0625, 22.4844],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "similarity_wrong_class_0: tensor([23.8750, 21.0625, 23.5000, 23.2344, 23.7500, 21.6406, 23.5312, 22.1094,\n",
      "        23.0000, 22.0312, 23.5000, 21.0469, 22.7344, 22.1875, 22.2812, 20.5938,\n",
      "        18.0781, 22.6250, 19.4219, 22.2344, 16.8438, 19.9062, 23.5000, 22.4062,\n",
      "        17.9844, 22.9688, 23.1250, 21.6250, 19.9062, 21.6875, 21.9219, 23.0156,\n",
      "        24.3125, 21.4062, 19.7031, 20.0312, 22.8281, 20.4844, 20.7031, 18.2656],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "similarity_wrong_class_0: tensor([24.2656, 21.7344, 23.7500, 23.8594, 24.1719, 22.0156, 23.9062, 22.7031,\n",
      "        23.0625, 22.5156, 24.0312, 21.3594, 23.3281, 22.5938, 22.0000, 21.4062,\n",
      "        18.4844, 22.4219, 19.9375, 22.9375, 17.2812, 20.5312, 23.8906, 22.7500,\n",
      "        18.6719, 23.7812, 23.4531, 21.1250, 19.8438, 22.5469, 22.7188, 23.1875,\n",
      "        22.5156, 21.7031, 19.8750, 20.4844, 22.0781, 20.7031, 21.4062, 18.3750],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "Result for class : female\n",
      "Keyword                     Score      Acc.  Bias\n",
      "----------------------  ---------  --------  ------\n",
      "black person             1.79688   0.8       S\n",
      "black                    0.75      0.655172  S\n",
      "boy                      0.5       0.611111  M\n",
      "born                     0.28125   0.567308  M\n",
      "native                   0.203125  0.75      S\n",
      "undated                  0.0625    0.703704  S\n",
      "student                 -0.0625    0.613793\n",
      "hospital                -0.109375  0.580645\n",
      "mother                  -0.171875  0.657895\n",
      "tooth                   -0.171875  0.633333\n",
      "police                  -0.21875   0.428571\n",
      "girl                    -0.25      0.568182\n",
      "found by police         -0.296875  0.419355\n",
      "rare                    -0.3125    0.578512\n",
      "person to die           -0.328125  0.678571\n",
      "disease                 -0.34375   0.543478\n",
      "genetic condition       -0.375     0.576577\n",
      "condition               -0.375     0.577181\n",
      "girl smiles             -0.390625  0.625\n",
      "person                  -0.390625  0.625253\n",
      "car                     -0.40625   0.605505\n",
      "head                    -0.40625   0.576577\n",
      "woman                   -0.421875  0.684466\n",
      "bedroom                 -0.4375    0.60241\n",
      "covered                 -0.453125  0.540541\n",
      "diagnosed               -0.484375  0.627737\n",
      "camera                  -0.515625  0.635135\n",
      "face                    -0.53125   0.64\n",
      "rare genetic            -0.59375   0.576577\n",
      "genetic                 -0.59375   0.576577\n",
      "rare genetic condition  -0.625     0.576577\n",
      "shot                    -0.625     0.527273\n",
      "found                   -0.671875  0.601881\n",
      "bathroom                -0.6875    0.595745\n",
      "smiles                  -0.703125  0.679487\n",
      "hair                    -0.703125  0.723404\n",
      "nose                    -0.796875  0.6\n",
      "broken                  -0.8125    0.602564\n",
      "person was found        -0.8125    0.625\n",
      "broken nose             -0.859375  0.631579\n"
     ]
    }
   ],
   "source": [
    "%cd /content/FACT/b2t\n",
    "print(\"fairfaces group 3 as minority, 3% like waterbirds. NORMAL METHOD\")\n",
    "\n",
    "\n",
    "# Define the Pipenv command and script\n",
    "command = \"python b2t.py --dataset fairfaces  --model best_model_Waterbirds_erm.pth --minority_group 3 --minority_ratio 0.03 --override_result\"\n",
    "# Run the command using the Pipenv environment\n",
    "!pipenv run bash -c \"{command}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 531241,
     "status": "ok",
     "timestamp": 1738354445298,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -60
    },
    "id": "4z4JgF-GzSXM",
    "outputId": "06bf7ed3-4edf-466c-f0c6-cea92f4b1fb8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/FACT/b2t\n",
      "Imagenet, val set for bee, ant and boar OUR METHOD\n",
      "bee\n",
      "('n02206856', 309, 'bee')\n",
      "ant\n",
      "('n02219486', 310, 'ant, emmet, pismire')\n",
      "boar\n",
      "('n02396427', 342, 'wild boar, boar, Sus scrofa')\n",
      "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n",
      "100% 97.8M/97.8M [00:00<00:00, 159MB/s]\n",
      "Pretrained model \"Resnet50\" loaded\n",
      "100% 196/196 [08:00<00:00,  2.45s/it]\n",
      "# of correct examples :  38073\n",
      "# of wrong examples :  11927\n",
      "# of all examples :  50000\n",
      "Accuracy : 76.15 %\n",
      "Classified result stored\n",
      "image_path: data/imagenet/data/imagenet-val/n02206856/\n",
      "Running scoring for bee\n",
      "progress at 0/3\n",
      "Start calculating scores..\n",
      "df_wrong_class_1[image] count: 10\n",
      "df_correct_class_1[image] count: 40\n",
      "keywords_class_1 count: 26\n",
      "b2t.py:314: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_wrong_class_1['updated_image'] = df_wrong_class_1['image'].apply(\n",
      "Processing images: 100% 1/1 [00:00<00:00,  4.10it/s]\n",
      "got heere 11\n",
      "b2t.py:323: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_correct_class_1['updated_image'] = df_correct_class_1['image'].apply(\n",
      "Processing images: 100% 1/1 [00:00<00:00,  1.82it/s]\n",
      "got heere\n",
      "Result for class : bee\n",
      "Keyword                 Score      Acc.  Bias\n",
      "------------------  ---------  --------  ------\n",
      "roof                 1.79688   0         S\n",
      "beehive              1.78125   0         S\n",
      "roof top             1.375     0         S\n",
      "sunflower            1.32812   0         S\n",
      "glass jar            1.26562   0         S\n",
      "jar                  1.20312   0         S\n",
      "glass                0.609375  0         S\n",
      "plants               0.5       0         S\n",
      "yellow               0.4375    0.875     S\n",
      "top                  0.328125  0         S\n",
      "biological           0.125     0         S\n",
      "biological genus     0.03125   0         S\n",
      "wild                -0.03125   0\n",
      "flowering plants    -0.078125  0\n",
      "genus of flowering  -0.328125  0\n",
      "released            -0.375     0\n",
      "flowering           -0.390625  0\n",
      "close               -0.390625  0.5\n",
      "white               -0.484375  0.5\n",
      "bees                -0.53125   0.2\n",
      "yellow flower       -0.5625    0.875\n",
      "genus               -0.609375  0\n",
      "white flower        -0.90625   0.5\n",
      "fly                 -0.9375    0.8\n",
      "flower              -1.20312   0.891892\n",
      "bee                 -1.75      0.875\n",
      "image_path: data/imagenet/data/imagenet-val/n02219486/\n",
      "Running scoring for ant, emmet, pismire\n",
      "progress at 1/3\n",
      "Start calculating scores..\n",
      "df_wrong_class_1[image] count: 10\n",
      "df_correct_class_1[image] count: 40\n",
      "keywords_class_1 count: 40\n",
      "b2t.py:314: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_wrong_class_1['updated_image'] = df_wrong_class_1['image'].apply(\n",
      "Processing images: 100% 1/1 [00:00<00:00,  7.81it/s]\n",
      "got heere 11\n",
      "b2t.py:323: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_correct_class_1['updated_image'] = df_correct_class_1['image'].apply(\n",
      "Processing images: 100% 1/1 [00:00<00:00,  2.13it/s]\n",
      "got heere\n",
      "Result for class : ant, emmet, pismire\n",
      "Keyword                        Score      Acc.  Bias\n",
      "-------------------------  ---------  --------  ------\n",
      "black metal bracelets       2.6875    0         S\n",
      "eyed moth                   2.34375   0         S\n",
      "butterfly                   1.92188   0         S\n",
      "bracelets                   1.92188   0         S\n",
      "metal bracelets             1.84375   0         S\n",
      "butterfly that person       1.71875   0         S\n",
      "patch                       1.60938   0         S\n",
      "white worm                  1.57812   0         S\n",
      "moth                        1.54688   0         S\n",
      "small patch                 1.17188   0         S\n",
      "black and white             0.984375  0         S\n",
      "white vector illustration   0.6875    0         S\n",
      "person found                0.5       0         S\n",
      "white background            0.5       0         S\n",
      "leaf                        0.5       0.75      S\n",
      "found                       0.4375    0.5       S\n",
      "person                      0.40625   0         S\n",
      "black                       0.375     0.5       S\n",
      "white                       0.359375  0         S\n",
      "black metal                 0.34375   0         S\n",
      "small                       0.296875  0.5       S\n",
      "bee                         0.265625  0.75      S\n",
      "insect was found            0.171875  0         S\n",
      "white vector                0.15625   0         S\n",
      "water                       0.109375  0         S\n",
      "metal                       0.0625    0         S\n",
      "flower                      0.046875  0.833333  S\n",
      "ladybug                    -0.046875  0\n",
      "red                        -0.0625    0.75\n",
      "eyed                       -0.125     0\n",
      "tiny                       -0.28125   0.5\n",
      "background                 -0.515625  0.5\n",
      "tiny tiny                  -0.53125   0\n",
      "grass                      -0.59375   0\n",
      "insect                     -0.609375  0\n",
      "pair                       -0.671875  0\n",
      "patch of grass             -0.6875    0\n",
      "close                      -1.09375   0.9\n",
      "tiny insect                -1.8125    0\n",
      "tiny tiny insect           -2.15625   0\n",
      "image_path: data/imagenet/data/imagenet-val/n02396427/\n",
      "Running scoring for wild boar, boar, Sus scrofa\n",
      "progress at 2/3\n",
      "Start calculating scores..\n",
      "df_wrong_class_1[image] count: 7\n",
      "df_correct_class_1[image] count: 43\n",
      "keywords_class_1 count: 22\n",
      "b2t.py:314: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_wrong_class_1['updated_image'] = df_wrong_class_1['image'].apply(\n",
      "Processing images: 100% 1/1 [00:00<00:00,  5.45it/s]\n",
      "got heere 11\n",
      "b2t.py:323: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_correct_class_1['updated_image'] = df_correct_class_1['image'].apply(\n",
      "Processing images: 100% 1/1 [00:00<00:00,  2.12it/s]\n",
      "got heere\n",
      "Result for class : wild boar, boar, Sus scrofa\n",
      "Keyword              Score      Acc.  Bias\n",
      "---------------  ---------  --------  ------\n",
      "bull elephant     3.40625   0         S\n",
      "elephant          2.82812   0         S\n",
      "wildebeest        1.98438   0         S\n",
      "ponies            1.82812   0.5       S\n",
      "wild ponies       1.82812   0.5       S\n",
      "bull              1.76562   0         S\n",
      "pair              1.32812   0.5       S\n",
      "calf              0.921875  0         S\n",
      "pair of wild      0.875     0.5       S\n",
      "zoo               0.84375   0.75      S\n",
      "swamp             0.796875  0         S\n",
      "black             0.40625   0.666667  S\n",
      "bear              0.359375  0.666667  S\n",
      "black bear        0.046875  0.5       S\n",
      "free             -0.0625    0\n",
      "living           -0.15625   0\n",
      "pen              -0.203125  0.5\n",
      "free and living  -0.390625  0\n",
      "wild             -0.421875  0.947368\n",
      "goat             -0.4375    0\n",
      "pigs             -0.59375   0.5\n",
      "pig              -1.3125    0.941176\n"
     ]
    }
   ],
   "source": [
    "%cd /content/FACT/b2t\n",
    "# Define the Pipenv command and script\n",
    "print(\"Imagenet, val set for bee, ant and boar OUR METHOD\")\n",
    "\n",
    "command = \"python b2t.py --dataset imagenet --model Resnet50 --class_names bee ant boar --extract_caption\"\n",
    "\n",
    "# Run the command using the Pipenv environment\n",
    "!pipenv run bash -c \"{command}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ImAh935H338X"
   },
   "source": [
    "## Label Diagnosis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RUtQdMUD3wCd"
   },
   "source": [
    "The LabelDiagnoser gives a csv with called labelDiagnosis.csv. In this file there are cols with the word used for the search (\"bee\", \"boar\", \"desk\", \"market\"), there is a col with the Actual class, there is a col with the caption and finally a col with the file name so you can find the images manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14289,
     "status": "ok",
     "timestamp": 1738354459580,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -60
    },
    "id": "cT67lH0S3u3H",
    "outputId": "f2dda2be-bd2e-469f-ff91-46e4f7ddb436"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/FACT\n",
      "Processing files: 100% 50000/50000 [00:13<00:00, 3840.68file/s] \n",
      "Saved results to labelDiagnosis.csv with 472 entries.\n"
     ]
    }
   ],
   "source": [
    "%cd /content/FACT\n",
    "# Define the Pipenv command and script\n",
    "command = \"python LabelDiagnoser.py\"\n",
    "\n",
    "# Run the command using the Pipenv environment\n",
    "!pipenv run bash -c \"{command}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WtVxP76WEt28"
   },
   "source": [
    "## Part of the debias training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 44392,
     "status": "ok",
     "timestamp": 1738357473520,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -60
    },
    "id": "aogDHOxTEqRY",
    "outputId": "054332e8-92b4-470c-d187-119920246274"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/FACT/b2t/b2t_debias\n",
      "/root/.local/share/virtualenvs/FACT-Z5lTYn3d/lib/python3.8/site-packages/torch/utils/data/dataloader.py:563: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "100% 19/19 [00:27<00:00,  1.43s/it]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.98      0.98      4555\n",
      "           1       0.66      0.87      0.75       240\n",
      "\n",
      "    accuracy                           0.97      4795\n",
      "   macro avg       0.83      0.92      0.87      4795\n",
      "weighted avg       0.98      0.97      0.97      4795\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%cd /content/FACT/b2t/b2t_debias\n",
    "# Define the Pipenv command and script\n",
    "command = \"python infer_group_label.py\"\n",
    "\n",
    "# Run the command using the Pipenv environment\n",
    "!pipenv run bash -c \"{command}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SkkpKzwCE4FD"
   },
   "source": [
    "## Zero shot prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 77769,
     "status": "ok",
     "timestamp": 1738355811494,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -60
    },
    "id": "qlrqro8MEvvY",
    "outputId": "31b25984-ac51-4ffe-d317-36aca8ba1470"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/FACT/b2t/b2t_debias\n",
      "100%|███████████████████████████████████████| 244M/244M [00:22<00:00, 11.1MiB/s]\n",
      "/root/.local/share/virtualenvs/FACT-Z5lTYn3d/lib/python3.8/site-packages/torch/utils/data/dataloader.py:563: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "example text: a photo of a landbird with woods.\n",
      "example text: a photo of a waterbird with woods.\n",
      "100% 23/23 [00:33<00:00,  1.45s/it]\n",
      "total 5794\n",
      "total corrects: tensor(4430)\n",
      "calculating accuracies\n",
      "group:  0\n",
      "total in group 2255\n",
      "correct in group 2108\n",
      "group:  1\n",
      "total in group 2255\n",
      "correct in group 1375\n",
      "group:  2\n",
      "total in group 642\n",
      "correct in group 384\n",
      "group:  3\n",
      "total in group 642\n",
      "correct in group 563\n",
      "worst group accuracy: 59.81308411214953\n",
      "average accuracy: 76.45840524680705\n"
     ]
    }
   ],
   "source": [
    "%cd /content/FACT/b2t/b2t_debias\n",
    "# Define the Pipenv command and script\n",
    "command = \"python prompt.py\"\n",
    "\n",
    "# Run the command using the Pipenv environment\n",
    "!pipenv run bash -c \"{command}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 50292,
     "status": "ok",
     "timestamp": 1738355931504,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -60
    },
    "id": "IapAgTzQFQJo",
    "outputId": "31376893-c60e-4f59-b633-731fbef97ce5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/FACT/b2t/b2t_debias\n",
      "/root/.local/share/virtualenvs/FACT-Z5lTYn3d/lib/python3.8/site-packages/torch/utils/data/dataloader.py:563: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "example text: a photo of a bald landbird.\n",
      "example text: a photo of a bald waterbird.\n",
      "100% 23/23 [00:36<00:00,  1.57s/it]\n",
      "total 5794\n",
      "total corrects: tensor(4312)\n",
      "calculating accuracies\n",
      "group:  0\n",
      "total in group 2255\n",
      "correct in group 2154\n",
      "group:  1\n",
      "total in group 2255\n",
      "correct in group 1225\n",
      "group:  2\n",
      "total in group 642\n",
      "correct in group 356\n",
      "group:  3\n",
      "total in group 642\n",
      "correct in group 577\n",
      "worst group accuracy: 54.32372505543237\n",
      "average accuracy: 74.4218156713842\n"
     ]
    }
   ],
   "source": [
    "%cd /content/FACT/b2t/b2t_debias\n",
    "# Define the Pipenv command and script\n",
    "command = \"python prompt.py --score negative\"\n",
    "\n",
    "# Run the command using the Pipenv environment\n",
    "!pipenv run bash -c \"{command}\""
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [
    {
     "file_id": "https://github.com/Joeyjdl/FACT/blob/merging/b2t_experiments%20copy.ipynb",
     "timestamp": 1738357479922
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
